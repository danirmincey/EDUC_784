# Simple Regression {#chapter-2}

```{r, echo = F}
button <-  "position: relative; 
            top: -25px; 
            left: 85%;   
            color: white;
            font-weight: bold;
            background: #4B9CD3;
            border: 1px #3079ED solid;
            box-shadow: inset 0 1px 0 #80B0FB"
```

The focus of this course is linear regression with multiple predictors (AKA *multiple regression*), but we start by reviewing regression with one predictor (AKA *simple regression*). 

## An example from NELS {#example-2}
 
```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

Figure \@ref(fig:fig1) shows the relationship between Grade 8 Reading Achievement (percent correct on a reading test) and Socioeconomic Status (SES; a composite measure on a scale from 0-35). The data are a subsample of the 1988 National Educational Longitudinal Survey (NELS; see https://nces.ed.gov/surveys/nels88/). 

```{r fig1, fig.cap = 'Reading Achievement and SES (NELS88).', fig.align = 'center'}
# Load and attach the NELS88 data
load("NELS.RData")
attach(NELS)

# Scatter plot
plot(x = ses, y = achmat08, col = "#4B9CD3", ylab = "Reading Achievement (Grade 8)", xlab = "SES")

# Run the regression model
mod <- lm(achmat08 ~ ses)

# Add the regression line to the plot
abline(mod) 
```

The strength and direction of the linear relationship between the two variables is summarized by their correlation (specifically, the Pearson product moment correlation). In this sample, the correlation is

```{r}
options(digits = 2)
cor(achmat08, ses)
```

This is a moderate, positive correlation between Reading Achievement and SES. This correlation means that eighth graders from more well-off families (higher SES) also tended to do better in reading (higher Reading Achievement). 

This relationship has been widely documented and discussed in education research (e.g., https://www.apa.org/pi/ses/resources/publications/education). Please share your thoughts on this relationship in class. 

<!-- In terms of Figure \@ref(fig:fig1), the magnitude (strength) of the correlation can be interpreted in terms of how close the points are to the line. If the correlation was exactly one, all of the points would be on the line. If the correlation was zero, the points would be scattered without any (linear) trend at all. See the review materials in Section \@ref(Review) for more info about correlation. -->

## The regression line {#regression-line}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

The line in the Figure \@ref(fig:fig1) can be represented mathematically as   

\[ 
\widehat Y = a + b X
(\#eq:yhat)
\]

where 

* $Y$ denotes Reading Achievement
* $X$ denotes SES 
* $\widehat Y$ represented the values of $Y$ on the line  
* $a$ represents the regression intercept (the value of $\widehat Y$ when $X = 0$)
* $b$ represents the regression slope (how much $\widehat Y$ increases for each unit of increase in $X$)

Note that $Y$ represents the values of Reading Achievement in the data, whereas $\widehat Y$ represents the values on the regression line. The difference $e = Y - \widehat Y$ is called a *residual*. The residuals for a subset of the data points in Figure \@ref(fig:fig1) are shown in pink in Figure \@ref(fig:fig2) 

```{r fig2, fig.cap = 'Residuals for a Subsample of the Example.', fig.align = 'center'}
# Get predicted values from regression model
yhat <- mod$fitted.values

# select a subset of the data
set.seed(10)
index <- sample.int(500, 30)

# plot again
plot(x = ses[index], y = achmat08[index], ylab = "Reading Achievement (Grade 8)", xlab = "SES")
abline(mod)

# Add pink lines
segments(x0 = ses[index], y0 = yhat[index] , x1 = ses[index], y1 = achmat08[index], col = 6, lty = 3)

# Overwrite dots to make it look at bit better
points(x = ses[index], y = achmat08[index], col = "#4B9CD3", pch = 16)
```

Notice that $Y = \widehat Y + e$ by definition. So, we can use either Equation \@ref(eq:yhat) or Equation \@ref(eq:y) to write out a regression model. 

\begin{align} 
Y = a + bX + e 
(\#eq:y)
\end{align} 

Both equations say the same thing, but Equation \@ref(eq:y) lets us talk about the values of $Y$ in the data, not just the predicted values. 

## OLS {#ols}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

Intuitively, one approach to “fitting a line to the data” is to select the parameters of the line (its slope and intercept) to minimize the residuals. In ordinary least squares (OLS) regression, we minimize a related quantity, the sum of squared residuals: 

\[
\begin{align} 
SS_{\text{res}} & = \sum_{i=1}^{N} e_i^2 \\ 
& = \sum_{i=1}^{N} (Y_i - a - b X_i)^2 
\end{align} 
\]

where $i = 1 \dots N$ indexes the respondents in the sample. OLS regression is very widely used and is the main focus of this course, although we will visit some other approaches (notably logistic regression) in the second half of the course. 

Solving the  minimization problem (i.e., doing the calculus) gives the following equations for the regression parameters

\[ 
a = \bar Y - b \bar X \quad \quad \quad \quad b = \frac{\text{Cov}(X, Y)}{s^2_X} = r_{XY} \frac{s_X}{s_Y}
\]

(If you aren't familiar with the symbols in these equations, check out the review materials in Section \@ref(Review) for a refresher.)  

For the NELS example, the regression intercept and slope are, respectively: 

```{r}
coef(mod)
```

**Please provide an interpretation of these numbers in terms of the line in Figure \@ref(fig:fig1), and be prepared to share your answers in class!** 

### Correlation and regression

Note that if $X$ and $Y$ are transformed to z-scores (i.e., to have mean of zero and variance of one), then 

* $a = 0$
* $b = \text{Cov}(X, Y) = r_{XY}$

So, regression, correlation, and covariance are all very closely related in when we only consider two variables at a time. This is why we didn't make a big deal about simple regression in EDUC 710. But when we get to multiple regression (i.e., more than one $X$ variable), we will see that relationship between regression and correlation (and covariance) gets more complicated.  

## R-squared
```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```
In the previous section we saw that the predicted value of Educational Achievement increased by .43 units (about half a percentage point) for each unit of increase in SES. Another way to interpret this relationship is in terms of the proportion of variance in Reading Achievement that is associated with SES -- i.e., to what extent are individual differences in Reading Achievement associated with, or explained by, individual differences in SES? 

This question is represented graphically in Figure \@ref(fig:rsquared). The horizontal line denotes the mean of Reading Achievement. The difference between the indicated student's Reading Achievement score and the mean can be divided into two parts. The black dashed line shows how much closer we get to the student's score by considering $\widehat Y$ instead of $\bar Y$. This represents the extent to which this student's Reading Achievement score is explained by the linear relationship with SES. The pink part is the regression residual, which was introduced in Section \@ref(regression-line). This is the variation in Reading Achievement that is "left over" after considering the linear relationship with SES. 


```{r rsquared, fig.cap = 'The Idea Behind R-squared.', fig.align = 'center', echo = F}
plot(x = ses[index], y = achmat08[index], ylab = "Reading Achievement (Grade 8)", xlab = "SES")
abline(mod)

ybar <- mean(achmat08)
abline(h = ybar, col = "gray")

# Add pink lines for case 6
segments(x0 = ses[index[22]], y0 = yhat[index[22]] , x1 = ses[index[22]], y1 = achmat08[index[22]], col = 6, lty = 3, lwd = 2)

# Add black lines 
segments(x0 = ses[index[22]], y0 = yhat[index[22]] , x1 = ses[index[22]], y1 = ybar, col = 1, lty = 3, lwd = 2)

# Overwrite dots to make it look at bit better
points(x = ses[index], y = achmat08[index], col = "#4B9CD3", pch = 16)
```


The R-squared statistic summarizes the variation in Reading Achievement associated with SES (i.e., the black dashed line) relative to the total variation in Reading Achievement (i.e., black + pink) for all students in the sample. Aside from the regression parameters, R-squared is the most widely used statistic in regression analysis, so we will be seeing it a lot. Some authors call it the "coefficient of determination" instead of R-squared. 

Using all of the cases from the example (Figure \@ref(fig:fig1)), the R-squared statistic is: 

```{r}
options(digits = 5)
summary(mod)$r.squared
```

**Please write down an interpretation of this number and be prepared to share your answer in class!** 

### Derivation*

To derive the R-squared statistic we work the numerator of the variance, which is called the total sum of squares.  

\[SS_{\text{total}} = \sum_{i = 1}^N (Y - \bar Y)^2. \]

It can be re-written using the predicted values $\widehat Y$:

\[SS_{\text{total}} = \sum_{i = 1}^N [(Y - \widehat Y) + (\widehat Y - \bar Y)]^2. \]

The right hand side can be reduced to two other sums of squares using the rules of summation algebra (see the review in Section \@ref(review)):

\begin{align} 
SS_{\text{total}} & = \sum_{i = 1}^N (Y - \widehat Y)^2 + \sum_{i = 1}^N (\widehat Y - \bar Y)^2 \\
\end{align} 

The first part is just $SS_\text{res}$ from Section \@ref(ols). The second part is called the regression sum of squares and denoted $SS_\text{reg}$. Using this terminology we can re-write the above equation as

\[ SS_{\text{total}} = SS_\text{res} + SS_\text{reg} \] 

The R-squared statistic is 

\[R^2 = SS_{\text{reg}} / SS_{\text{total}}. \]

As discussed above, this is interpreted as the proportion of variance in $Y$ that is explained by its linear relationship with $X$. 

<!-- ### Additional details* 

$R$ is the correlation between $Y$ and $\widehat Y$. In simple regression, this is just equal to the regular correlation coefficient. 

\[ R = \text{Cor}(Y, \widehat Y) = \text{Cor}(Y, a + bX) = \text{Cor}(Y, X)\]

(You can prove this result using the rules of summation algebra.) 

Consequently, in simple regression $R^2 = r^2_{XY}$. When we add multiple predictors, $R$ is called the multiple correlation and plays a very important -->


## The population model {#population-model}  

In the NELS example, the population of interest is U.S. eighth graders in 1988. We want to be able to draw conclusions about that population based on the sample of eighth graders that participated in NELS. In order to do that, we make some statistical assumptions about the population, which are collectively referred to as the population model. We talk about how to check the plausibility of these assumptions in Chapter \@ref(chapter-x). 

The regression population model has the following three assumptions, which are also depicted in the diagram below. Recall that the notation $Y \sim N(\mu, \sigma)$ means that a variable $Y$ has a normal distribution with mean $\mu$ and standard deviation $\sigma$.

1. Normality: The distribution of $Y$ conditional on $X$ is normal for all values of $X$. 

\[Y | X \sim  N(\mu_{Y | X} , \sigma_{Y | X}) \]

2. Homoskedasticity: The conditional distributions have equal variances (also called homegeneity of variance).

\[ \sigma_{Y| X} = \sigma \]

3. Linearity: The means of the conditional distributions are a linear function of $X$.

\[ \mu_{Y| Χ} = a + bX \]

```{r, fig-pop-model, echo = F, fig.cap = "The Regression Population Model.", fig.align = 'center'}
knitr::include_graphics("images/population_model.png")
```

These three assumptions are summarized by writing 

\[ Y|X \sim N(a + bX, \sigma). \] 

Sometimes it will be easier to state the assumptions in terms of the population residuals, $\epsilon = Y - \mu_{Y|X}$, which subtract off the regression line: $\epsilon \sim N(0, \sigma)$. 

An additional assumption is usually made about the data in the sample -- that they were obtained as a simple random sample from the population. We will see some ways of dealing with other types of samples later on the course (e.g., Chapter \@ref(chapter-x)), but for now we can consider this a background assumption that applies to all of the procedures discussed in this course.  

## Clarifying notation

At this point we have used the mathematical symbols for regression (e.g., $a$, $b$) in two different ways:

* In Section \@ref(regression-line) they denoted sample statistics.
* In Section  \@ref(population-model) they denoted population parameters.

The population versus sample notation for regression is a bit of a hot mess, but the following conventions are widely used.

Concept                Sample statistic    Population parameter               
-------               ------------------  ----------------------
regression line         $\widehat Y$          $\mu_{Y|X}$ 
slope                   $\widehat b$          $b$                                 
intercept               $\widehat a$          $a$                                 
residual                $e$                   $\epsilon$                           
variance explained      $\widehat R^2$        $R^2$


The "hats" always denote sample quantities, and the Greek letters always denote population quantities, but there is some lack of consistency. For example, why not use $\beta$ instead of $b$ for the population slope? Well, $\beta$ is conventionally used to denote standardized regression coefficients in the *sample*, so its already taken  (more on this in the Chapter \@ref(chapter-4) ). 

One thing to note is that the hats are usually omitted from the statistics $\widehat a$,  $\widehat b$, and $\widehat R^2$ if it is clear from context that we are talking about the sample rather than the population. This doesn't apply to $\widehat Y$, because the hat is required to distinguish the predicted values from the data points. 

Another thing to note is that while $\widehat Y$ is often called the predicted value(s), $\mu_{Y|X}$ is not usually referred to this way. It is called the conditional mean function or the conditional expectation function. 

<!-- Finally, recall that the symbol $E(\cdot)$ denotes the expected value of whatever statistic appears in the place of the dot. The expected value is a way of referring to the mean of the sampling distribution of the statistic (otherwise we would have to say things like "the mean of the mean", which would be horrible). So $E(Y\mid X)$ is the expected value of $Y$ for a given value of $X$. The $\mu_{Y|X}$ is the the population conditional mean, $\mu_{Y|X}$ is equal to $E(Y\mid X)$ when the population model is true (i.e., OLS regression is an unbiased estimator of the population conditional mean function). So, the two symbols are used interchangeably. (Omit this paragraph?) -->


## Inference for the slope {#inference-for-slope}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

When the population model is true, $\widehat b$ is an unbiased estimate of $b$. We also know the standard error of $\widehat b$, which is equal to (cite)

\[ s_{\widehat b} = \frac{s_Y}{s_X} \sqrt{\frac{1-R^2}{N-2}} . \]

Using these two results, we can compute t-tests and confidence intervals for the regression slope in the usual way. These are summarized below. See the review in Section \@ref(review) for background information on bias, precision, t-tests, and confidence intervals.

### t-tests

The null hypothesis $H_0: \widehat b = b_0$ can be tested against the alternative $H_A: \widehat b \neq b_0$ using the test statistic: 

\[ t = \frac{\widehat b - b_0}{s_{\widehat b}} \]

which has a t-distribution on $N-2$ degrees of freedom when the null hypothesis is true. 

The test assumes that the population model is correct. The null hypothesis value of the parameter is usually chosen to be $b_0 = 0$, in which case the test is interpreted in terms of the "statistical significance" of the regression slope.  

### Confidence intervals

For a given Type I Error rate, $\alpha$, the corresponding $(1-\alpha) \times 100\%$ confidence interval is

\[ b_0 = \widehat b \pm t_{(1-\alpha/2)} \times s_{\widehat b} \]

where $t_{(1-\alpha/2)}$ denotes the $1 - \alpha/2$ quantile of the $t$-distribution with $N-2$ degrees of freedom. 

For example, if $\alpha$ is chosen to be $.05$, the corresponding $95\%$ confidence interval uses $t_{(.025)}$, the 2.5-th percentile of the t-distribution. 

### The NELS example 

For the NELS example, the t-test of the regression slope is shown in the second row of the table below (we cover the rest of the output in the next few sections):

```{r}
summary(mod)
```

The corresponding $95\%$ confidence interval is

```{r}
confint(mod)
```

**Please write down an interpretation of the t-test and confidence interval of the regression slope, and be prepared to share your answers in class!** 

## Inference for the intercept

The situation for the regression intercept is similar to that for the slope: the OLS estimate is unbiased and its standard error is (cite)

<!--- check this and get source -->
\[ 
s_{\widehat a} = \sqrt{\frac{SS_{\text{res}}}{N-2} \left(\frac{1}{N} + \frac{\bar X^2}{(N-1)s^2_X}\right)}.
\]

The t-tests and confidence intervals are constructed in the way same as for the slope, with $a$ replacing $b$ in the notation of the previous slide. The t-distribution also has $N-2$ degrees of freedom for the intercept.  

It is not usually the case that the regression intercept is of interest in simple regression. Recall that the intercept is the value of $\widehat Y$ when $X = 0$. So, unless you have a hypothesis or research question about this particular value of $X$ (e.g., eighth graders with $SES = 0$), there isn't a good rationale for testing the regression intercept.  

When we get to multiple regression, we will see some examples of regression models where the intercept is meaningful, especially when we talk about categorical predictors in Chapter \@ref(categorical-predictors) and interactions in Chapter \@ref(interactions). But, for now, we can put it on the back burner. 

**For sake of completeness, please take another look at the R output in the previous section and provide an interpretation of the t-test and confidence interval of the regression intercept, and be prepared to share your answers in class!** 


## Inference for R-squared {#inference-for-rsquared}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

Inference for R-squared is quite a bit different than for the regression parameters.  R-squared is a ratio of two sums of squares. We know from our study of ANOVA last semester that ratios of sums of squares are tested using an F-test, rather than a t-test. The F-test for (the population) R-squared is summarized below. 

### F-tests

The null hypothesis $H_0: R^2 = 0$ can be tested against the alternative $H_A: R^2 \neq 0$ using the test statistic: 

\[ F = (N-2) \frac{\widehat R^2}{1-\widehat R^2} \]

which has a F-distribution on $1$ and $N – 2$ degrees of freedom when the null is true. The test assumes that the population model is true. Confidence intervals for R-squared are generally not reported. 

The R output from Section \@ref(inference-for-slope) is presented again below. **Please write down an interpretation of the F-test of R-squared and be prepared to share your answers in class!** Note that R uses the terminology "multiple R-squared" to refer to R-squared. 


```{r}
summary(mod)
```
