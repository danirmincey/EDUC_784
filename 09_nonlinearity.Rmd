# Nonlinearity, etc {#chapter-9}

```{r, echo = F}
# Clean up check
rm(list = ls())
#detach(NELS)

button <-  "position: relative; 
            top: -25px; 
            left: 85%;   
            color: white;
            font-weight: bold;
            background: #4B9CD3;
            border: 1px #3079ED solid;
            box-shadow: inset 0 1px 0 #80B0FB"
```

In the previous chapter we considered how to check whether the population assumptions of linear regression are problematic for a given data set. In this chapter we discuss some ways to deal with assumption violations. 

One common way to  address assumption violations is to "transform" the $Y$ variable. For example, when the $Y$ variable has positive skew, the transformations $\log(Y)$ or $\sqrt Y$ can help reduce skew.  In general, a transformation is just a function we apply to variable to change it into another variable that we would rather work with. We have already seen one example of a transformation: the z-transformation or z-scores. In this chapter we will focus on the use and interpretation of the log-transform, which not only illustrates the overall approach of transforming the $Y$-variable, but also leads into our next topic -- logistic regression -- which also involves logarithms. 

Another approach for dealing with assumption violations is to transform one or more of the $X$-variables as well as or instead of the $Y$ variable. These approaches are usually focused specifically on addressing non-linearity, and we will consider two specific examples:

* polynomial regression, which means raising $X$-variables to an exponent (e.g., $X^2$ and $X^3$), and 
* piecewise or segmented regression, which involves using different regression lines over different ranges of a predictor. 

Both polynomial and piecewise regression are very useful in practice and lead to advanced topics like splines and semi-parametric regression. They also turn out to be special cases of interactions, so we have already covered a lot of the technical details in Chapter \@ref(chapter-6) -- phew! 

Whenever we transform a variable in a regression model, either $Y$ or $X$, it always has three types of consequences. It affects: 

* The distribution of the variable. 
* The (non-)linearity of its relationship with other variables.
* The interpretation of the coefficients the model.  

The big theme of this chapter is that we have to consider all three of things when transforming data. In particular, we might find that addressing one problem (e.g., non-normality) can create more problems that it solves (e.g., nonlinearity and unintepretable model parameters). In general, non-normality and nonlinearity are always treated as a pair because any transformation that affects one of them will also affect the other. 

Finally, we address heteroskedasticity. The short version is that heteroskedasticity does not affect the estimates of regression coefficients (i.e., the values reported in the first column of R's `summary` table), but it does affect their standard errors (the second column), and, consequently, the t- and p-values. There are lots of different ways to make standard errors robust to heteroskedasticity, and we will focus on one widely used procedure called heteroskedasticity-corrected (HC) standard errors.  

## Math review: Logarithms {#math-review-9}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

Before we get into transforming our variables, let's review the requisite math -- i.e., logs and exponents. We will use the lowercase symbols $x$ and $y$ for generic mathematical variables in this section -- these symbols do not correspond to the use of the uppercase symbols $X$ and $Y$ in regression notation. So, e.g., $x$ is not predictor, its just a generic variable. 

The function $\log_{10}(x)$ returns the power to which we need to raise 10 to get the value $x$. It answers the question: $10^? = x$. 

Some examples:

* $\log_{10}(10) = 1$, because $10^1 = 10$
* $\log_{10}(100) = 2$, because $10^2 = 100$
* $\log_{10}(1000) = 3$, because $10^3 = 1000$
* ...

Treated as a function of $x$, we can see that $\log(x)$ grows much more slowly than $x$. The three values computed above are marked by dashed lines in Figure \@ref(fig:logx). What this is telling us that "big differences" on $x$ translate into "small differences" on $log(x)$. For every order of magnitude that $x$ increases (e.g., from 10 to 100, or from 100 to 1000), its log only increases by one unit.  

```{r, logx, echo = F, fig.cap = "Log base 10", fig.align = 'center'}
knitr::include_graphics("images/logx.png", dpi = 150)
```

This "compression" of large values down to a smaller scale turns out to be useful for dealing with positive skew, as illustrated in Figure \@ref(fig:logdata). This is one reason that the log transform is widely used  -- but remember, this will also affect the (non)-linearity of its relationship with other variables and the interpretation of model parameters in which the variable is used!

```{r logdata, echo = F, fig.cap = "Logs and positive skew", fig.align = 'center'}
load("ECLS2577.Rdata")
attach(ecls)
par(mfrow = c(1,2))
hist(c1rmscal, col = "#4b9cd3")
hist(log(c1rmscal + 1), col = "#4b9cd3")
detach(ecls)
```

Now that we have seen the basic interpretation and utility of logs, there are just a few details to add. 

### Natural logs (ln)
The symbol $\log_{10}$ is read "log base 10". In statistics, we almost never use log base 10. Instead we use log base $e$ -- i.e., $\log_e$ -- where $e = 2.7182...$ is called Euler's numbers. Think of $e$ as a relative of $\pi$ (the ratio of circle's circumference to its diameter). It is an irrational number (i.e., cannot be represented as a ratio) and shows up all over the place. We will see later on that using $\log_e$ makes the interpretation of log-linear regression much simpler. Note that in this context, $e$ is not the residual from a regression model. The interpretation should be clear from context, but just ask a question in class if you are unsure. 

Regardless of which base function we use, logs have the same overall compression effect, as shown in Figure \@ref(fig:loge). For example, $\log_{10}(1000) = 3$ and $\log_{e}(1000) \approx 7$ -- both numbers are much smaller than $1000$! The same is true when applied to real data -- in fact, the left hand panel in Figure \@ref(fig:logdata) was computed using $\log_e$. 

```{r, loge, echo = F, fig.cap = "Log base 10 vs log base e", fig.align = 'center'}
knitr::include_graphics("images/loge.png", dpi = 150)
```

Log base $e$, or $\log_e$,  is called the "natural logarithm" and often denoted $\ln$. But $\ln$ can be hard to read so we will just stick with $\log$ and omit the base symbol when we mean $\log_e$. This is consistent with what R's `log` function does -- its default base is $e$.

### log and exp

Logarithms have the property that they can always be undone -- they are invertible. For example, if we start with the number $x= 100$, we can take the log (base $e$) of that number as above

\[\log(100) =  4.60517 \]

Remember that by definition, this just tells us that the $4.60517$ is the power to which we have to raise $e$ in order get $100$, i.e., 

\[e^{4.60517} = 100 \]

In general, it is true (by definition) that if $\log(x) = y$ then $e^y = x$. So, exponentiation (i.e., raising something to a power) is the "opposite" or inverse of taking a logarithm. This is why logarithms and exponents always show up together -- if you are working with one, the other one will inevitably crop up at some point. 

From a statistical perspective, the important part about logs and exponents is that we can always use one to "undo" the effects of the other. So, if we log transform our $Y$ variable in a regression analysis to make the data more compatible with the model assumptions, we can always undo the log transform afterwards by taking exponents. We will see this trick a lot in what follows -- apply a transform to the data, run the model, then undo the transform to interpret the results. 

Since we are going to see a lot of exponents in the following chapters, they get the abbreviation "exp". We often use $\exp$ in mathematical notation to avoid having to write complicated expressions in the superscript. The idea is that the left hand side of this equation is easier to read than the right hand side.  

\[ \exp(y + 17 - choclate + banna ) = e^{y + 17 - choclate + banna} \]

Like logs, we assume that $\exp$ is using base $e$ unless otherwise noted. 

## Log-linear regression

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

There are a number of ways to apply logs in regression analysis, which are outlined in the following table and discussed in more detail in the supplemental readings for this week. The focus of this section is log-linear models. Note that the term "log-linear" is also the name of a family of models for contingency table data -- that is not what we are talking about here. 

```{r, log-table, echo = F, fig.cap = "Logs in regression analysis", fig.align = 'center'}
knitr::include_graphics("images/log_table.png", dpi = 150)
```

As mentioned, whenever we transform a variable we are doing three interrelated things: 

1. Changing the distribution of the variable 
2. Changing the (non-)linearity of its relationship with other variables
3. Changing interpretation of the coefficients in regression models that use the variable. 

Since the log-linear models are often used to address positive skew in the outcome variable, we will start with point 1 and tackle them in order.

### Non-normality (positive skew)

Below are some examples of the log transform applied to positively skewed data. We can see a range of results, from "wow, that definitely worked" in the first example to, "better but not great" in the second two examples. These are realistic reflections of how a log transform works to address skew. We will discuss the data used for the third example in more detail in the following subsections, but for now it is sufficient to note that the plots for the third example show the hourly wages of a sample 200 individuals from 1985. 


```{r skew1, echo = F, fig.cap = "Example 1: Log transforming ECLS math", fig.align = 'center', fig.height=8}
attach(ecls)
par(mfrow = c(2,2))
hist(c1rmscal, col = "#4b9cd3")
hist(log(c1rmscal + 1), col = "#4b9cd3")
qqnorm(c1rmscal, col = "#4b9cd3", main = "qq-plot of c1rmscal" )
qqline(c1rmscal)
qqnorm(log(c1rmscal + 1), col = "#4b9cd3",  main = "qq-plot of log(c1rmscal + 1)" )
qqline(log(c1rmscal + 1))
```

```{r skew2, echo = F, fig.cap = "Example 2: Log transforming ECLS reading", fig.align = 'center', fig.height=8}
par(mfrow = c(2,2))
hist(c1rrscal, col = "#4b9cd3")
hist(log(c1rrscal + 1), col = "#4b9cd3")
qqnorm(c1rrscal, col = "#4b9cd3", main = "qq-plot of c1rrscal" )
qqline(c1rrscal)
qqnorm(log(c1rrscal + 1), col = "#4b9cd3",  main = "qq-plot of log(c1rrscal + 1)" )
qqline(log(c1rrscal + 1))
detach(ecls)
```

```{r skew3, echo = F, fig.cap = "Example 3: Log transforming hourly wages", fig.align = 'center', fig.height=8}
load("Wages.RData")
attach(wages)
par(mfrow = c(2,2))
hist(wage, col = "#4b9cd3")
hist(log(wage + 1), col = "#4b9cd3")
qqnorm(wage, col = "#4b9cd3", main = "qq-plot of wage" )
qqline(wage)
qqnorm(log(wage + 1), col = "#4b9cd3",  main = "qq-plot of log(wage + 1)" )
qqline(log(wage + 1))
detach(wages)
```

#### Why $\log(x+1)$?
There are a few things to note about this approach to correcting positive skew. First, you might wonder why we are computing $\log(x + 1)$ instead of $\log(x)$. This is because the log function has "weird" behavior for values of $X$ between 0 and 1. These values were not shown in Figures \@ref(fig:logx) and \@ref(fig:loge) where we discussed compression, but they are shown below in Figure \@ref(fig:log0). 

```{r, log0, echo = F, fig.cap = "Logs for small values of X", fig.align = 'center'}
knitr::include_graphics("images/log0.png", dpi = 150)
```

We can see that $\log(1) = 0$, which is true regardless of the base (i.e., $b^0 = 1$ for all choices of the $b$). But for values of $x < 0$, $\log(x)$ goes to negative infinity. This is because of how negative exponents are defined

\[ b^{-x} = \frac{1}{b^x} \]

The upshot of all this is that, if your variable takes on values in the range $[0, 1]$, the log transform is going to change those values into large negative numbers. This, in turn, can actually result in negative skew, rather fixing your positive skew. We can see this in Figure \@ref(fig:skew4) below. This is the same variable as Example 1 above (ECLS math), but this time we also show the variable transformed to a proportion before applying the log transform. In the left hand panels we can see that the distribution of the proportion is qualitatively the same the as the distribution of the original variable. However, the log transform behaves much differently for these two cases. Why? because logs are "weird" on the range $[0, 1]$. 

```{r skew4, echo = F, fig.cap = "Comparing Log transforms of ECLS math total scale versus proportion correct", fig.align = 'center', fig.height=8}
attach(ecls)
par(mfrow = c(2,2))
prop_c1rmscal <- (c1rmscal - min(c1rmscal))/100
hist(c1rmscal, col = "#4b9cd3")
hist(log(c1rmscal + 1), col = "#4b9cd3")
hist(prop_c1rmscal, col = "#4b9cd3")
hist(log(prop_c1rmscal), col = "#4b9cd3")
```

To avoid this situation I always add 1 to a variable before taking its log, that what I know that there are no values between 0 and 1. Technically, there is no reason to do this if the variable does not take on values in the range 0 to 1, but I do it just so I don't have to worry about it. I suggest you do the same. 

#### What about negative values of $x$?

You may have noted that the "always add 1 before taking logs" rule in the previous section doesn't cover cases where the variable can take on negative values. And you would be right! The problem here is that there is no way to turn a positive base $b$ into a negative number through exponentiation - i.e., the $\log(x)$ is undefined whenever $x < 0$. 

To address situations where the variable you want to transform can take on negative values, the appropriate transformation is 

\[ \log(x - \text{min}(x) + 1). \]

The following table shows how this transformation works

```{r, log-min, echo = F, fig.cap = "Dealing with negative values of x in log transforms", fig.align = 'center'}
knitr::include_graphics("images/log_min.png", dpi = 150)
```

The take home message: If we have negative values of $x$, before we apply the log transform we need to: 

Step 1. “Add” the minimum value of $x$ to prevent undefined values of log

Step 2. Add 1 to make sure no values appear in the interval (0, 1)

Like the "adding 1" rule, you can use this procedure all the time, not just when $x$ takes on negative values. 

#### Skew versus censoring

It is important to distinguish between skew and floor (or ceiling) effects. Floor and ceiling effects are collectively known as "censoring". Censoring looks like extreme skew, but is importantly different from skew because a large proportion of the observations have exactly the same value. 

For floor effects, a large proportion of cases have the minimum value, which can often look like positive skew. An example is shown in Figure \@ref(fig:floor) below. Note that the log transformation does do anything about censoring. In general, if you transform a "heap" of values, then the same heap shows up in the transformed data. For this type of data, you would need to consider alternatives to linear regression modeling (e.g., censored or tobit regression). The moral is: don't mistake censoring for skew, because they don't have the same solution. 

```{r, floor, echo = F, fig.cap = "Floor effect", fig.align = 'center'}
knitr::include_graphics("images/floor_effect.png", dpi = 125)
```

#### A final note on dealing with skew

Recall that the assumptions of linear regression are about the residuals, not about the $Y$ variable itself. So, a skewed outcome variable is not necessarily a problem. In particular, it is entirely possible that $Y$ is positively skewed but its residuals are normal, especially if one of the $X$ variables has a similar distribution to $Y$. So, don't worry about 

<!--
library(car)
library(lmtest)
mod
cov1 <- hccm(mod) 
mod1.HC <- coeftest(mod, vcov.=cov1)
coeftest(mod) -->

