#  Regression with Two Predictors {#chapter-4}

```{r, echo = F}
rm(list = ls())
button <-  "position: relative; 
            top: -25px; 
            left: 85%;   
            color: white;
            font-weight: bold;
            background: #4B9CD3;
            border: 1px #3079ED solid;
            box-shadow: inset 0 1px 0 #80B0FB"
```

## An example from ECLS {#ecls-4}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

The two plots below use a subset of data from the kindergarten class of 1998-99 cohort of the Early Childhood Longitudinal Survey (ECLS; https://nces.ed.gov/ecls/) to show the relationships among the following three variables: 

* Math Achievement in the first semester of Kindergarten. This variable can be interpreted as the number of questions out of 61 answered correctly on a math test. Don't worry -- the respondents in this study did not have to write a 61-question math test in the first semester of K! Students only answered a few of the questions and their scores were re-scaled to a total out of 61 afterwards. 

* Socioecomonic Status (SES), which is a composite of household factors (e.g., parental education, household income) ranging from 30-72. 

* Approaches to Learning (ATL), which is a teacher reported measure of behaviors that affect the ease with which children can benefit from the learning environment. It includes six items that rate the childâ€™s attentiveness, task persistence, eagerness to learn, learning independence, flexibility, and organization. The items have 4 response categories (1-4), coded so that higher values represent more positive responses, and the scale is an unweighted average the six items.

More details about these variables  are available in the ECLS user manual: https://nces.ed.gov/ecls/data/ECLSK_K8_Manual_part1.pdf.

```{r, pairs, fig.cap = 'ECLS Example Data.', fig.align = 'center'}
load("ECLS250.RData")
attach(ecls)
example_data <- data.frame(c1rmscal, wksesl, t1learn)
names(example_data) <- c("Math", "SES", "ATL")
pairs(example_data , col = "#4B9CD3")
```

In this plot, the panels are arranged in matrix format. The variable named on the diagonal appears on the vertical ($Y$) axis in its row and the horizontal ($X$) axis in its column. For example, Math Achievement is on the vertical axis in the first row and the horizontal axis in the first column. Notice that plots below the diagonal are a mirror image of the plots above the diagonal. 

The format of Figure \@ref(fig:pairs) is the same as that of the correlation matrix among the variables: 
```{r}
options(digits = 2)
cor(example_data)
```

Again, notice that the entries below the diagonal are mirrored by the entries above the diagonal. **If you have any questions about how these plots and correlations are presented, please write them down now and share them class!**

We can see that all three variables are positively correlated. SES and ATL have similar correlations with Math Achievement (.44 and .40, respectively), and are also moderately correlated with each other (.29). 

## The two-predictor model

In the ECLS example, we can think of Kindergarteners' Math Achievement scores as an outcome of interest, with SES and Approaches to Learning as potential predictors / explanatory variables. The multiple regression model for this example can be written as 

\[ 
\widehat Y = b_0 + b_1 X_1 + b_2 X_2 
(\#eq:yhat-4)
\]

where 

* $\widehat Y$ denotes the predicted Math Achievement scores
* $X_1 = \;$ SES and $X_2 = \;$ ATL (it doesn't matter which predictor we denote as $1$ or $2$)
* $b_1$ and $b_2$ are the regression slopes 
* The intercept is denoted by $b_0$ rather than $a$ -- this is just to make the notation consistent

Just like simple regression, the residuals for Equation \@ref(eq:yhat-4) are defined as $e = Y - \widehat Y$ and the model can be equivalently written as $Y = \widehat Y + e$. 

The correlations reported in Section \@ref(ecls-4) address how the three variables are (linearly) related in the ECLS data. Multiple regression lets us additionally address the following types of questions: 

* Does ATL "add anything" to our understanding of Math Achievement, beyond SES alone?
* What is the relative importance of the two predictors? 
* How much of the variance in Math Achievement do they explain? 

Before getting into the answers to these questions, let's first contrast multiple regression with simple regression. 

## Multiple vs simple regression

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

At first glance, it might appear that simple regression and multiple regression are essentially the same thing. However, there is an important ingredient in multiple regression that is missing from simple regression. This section illustrates how the two approaches differ and asks you to think about what the missing ingredient could be. 

Table \@ref(tab:compare) compares the output of three regression models using the ECLS example. 

* "Multiple" is a two-predictor model that regresses Math Achievement on SES and ATL.
* "Simple (SES)" regresses Math Achievement on SES only. 
* "Simple (ALT)" regresses Math Achievement on ALT only.  
  

```{r compare}
# Run models
mod1 <- lm(Math ~ SES  + ATL, data = example_data)
mod2a <- lm(Math  ~ SES, data = example_data)
mod2b <- lm(Math  ~ ATL, data = example_data)

# Collect output
out1 <- c(coef(mod1), summary(mod1)$r.squared)
out2a <- c(coef(mod2a), NA, summary(mod2a)$r.squared)
out2b <- c(coef(mod2b), NA, summary(mod2b)$r.squared)[c(1,3,2,4)]
out <- data.frame(rbind(out1, out2a, out2b))

# Clean up names
names(out) <- c(names(coef(mod1)), "R-squared")
out$Model <- c("Multiple", "Simple (SES)", "Simple (ATL)")
out <- out[c(5, 1:4)]
row.names(out) <- NULL

# Table 
options(knitr.kable.NA = '---')
knitr::kable(out, caption = "Regression Coefficients and R-squared From the Three Models")
```

There are two main things to notice about the table: 

* The regression coefficients in the multiple regression model do not equal the regression coefficients in the two simple regressions. This is an illustration of omitted variable bias, which we discussed in Section \@ref(causation).

* The proportion of variance explained (R-squared) in the two-predictor model is less than the sum of the proportion of variance explained in the two simple models. Why is this weird? Well, we know that total variance of Math Achievement isn't changing -- i.e., $SS_\text{total}$ is the same in all of the models. Therefore the R-squared values are all fractions with the same denominator, so they should be additive (e.g., $a/c + b/c = (a + b)/ c$). But the values in the table don't follow this pattern. 

In summary, the regression coefficients and R-squared values in the multiple regression model are different than what we would expect by extending simple regression in a naive way (i.e., by doing simple regression multiple times). 

### What is the missing ingredient? 

Recall that in simple regression, the regression slope is just a repackaging of the correlation between the outcome and predictor (see Section \@ref(ols)). So, 

* the "Simple (SES)" model considers the correlation between Math Achievement and SES, and 
* the "Simple ATL" model considers the correlation between Math Achievement and ATL. 

These two models leave out one of the correlations from Section \@ref(ecls-4) -- which one? 

Bonus: Explain why this constitutes a case of omitted variable bias. 

**Please write down your answers and be prepared to share them in class!**

## OLS with two predictors {#ols-4}

We can estimate the parameters of the two-predictor regression model in Equation \@ref(eq:yhat-4) model using same approach as for simple regression, by choosing the values of $b_0, b_1, b_2$ that minimize 

\[SS_\text{res} = \sum_i e_i^2.\]

This leads to the following equations for the regression coefficients (the subscripts $j = 1, 2$ denote $X_j$)

\begin{align}
b_0 & = \bar Y - b_1 \bar X_1 - b_2 \bar X_2 \\ \\ 
b_1 & = \frac{r_{Y1} - r_{Y2} r_{12}}{1 - r^2_{12}} \frac{s_1}{s_Y} \\ \\
b_2 & = \frac{r_{Y2} - r_{Y2} r_{12}}{1 - r^2_{12}} \frac{s_2}{s_Y}
(\#eq:2pred)
\end{align}


As promised, these equations are more complicated than for simple regression :) The next section addresses the interpretation of the regression coefficients.  

<!-- The simple regression model was a line in two dimensions, which was easy to visualize. The two-predictor models is technically a plane in 3 dimensions. This makes it harder to visual the model, especially as the number of predictors increases. But we can use various work arounds. For example, rather than plotting Y against X, we can plot the residuals against the fitted values as in Figure \@ref(fig:fig1-4). The pink lines in Figure \@ref(fig:fig1-4) have the same interpretation as in Figure \@ref(fig:fig2): they show how far the data points are form the predicted values. 

```{r fig1-4, fig.cap = 'Residuals for a Subsample of the Example.', fig.align = 'center'}
# Get predicted values from regression model
yhat <- mod1$fitted.values
res <- mod1$residuals

plot(x = yhat, y = res, ylab = "e", xlab = "Y-hat")
abline(h = 0)

# Add pink lines
segments(x0 = yhat, y0 = 0 , x1 = yhat, y1 = res, col = 6, lty = 2)

# Overwrite dots to make it look at bit better
points(x = yhat, y = res, col = "#4B9CD3", pch = 16)
```
-->

## Interpreting the coefficients {#interpretation-4}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

One of the most important aspects of using multiple regression analysis is getting the interpretation of the regression coefficients correct. The basic idea is that the slope parameter for SES represents how much the predicted Math Achievement scores change for a unit increase of SES, *while holding ATL constant.* (The same interpretation holds when switching the predictors.) The important difference with simple regression is the "holding the other predictor constant" part, so let's dig into it. 

### "Holding the other predictor constant"

We can start by revisitng the regression model in Equation \@ref(eq:yhat-4): 

\[ \widehat Y = b_0 + b_1 X_1 + b_2 X_2 \] 

If we increase SES ($X_1$) by one unit and hold ATL ($X_2$) constant, we get

\[ \widehat{Y^*} = b_0 + b_1 (X_1 + 1) + b_2 X_2. \]

The difference between $\widehat{Y^*}$ and $\widehat{Y}$ is how much the predicted value changes for a one unit increase in SES, while holding ATL constant: 

\[ \widehat{Y^*} - \widehat{Y}  = b_0\]

So, the interpretation of the coefficients in multiple regression as "holding the other predictor(s) constant" is an immediate consequence of the model. One draw back of this interpretation is that is might not "make sense" to hold one predictor constant while changing another -- in fact, if the predictors are correlated, this basically means that changes in one predictor are associated with changes in the other. There are other interpretation that address this issue directly. 

### "Controlling for the other predictor"

The equations in Section \@ref(ols-4) provide another interpretation in terms of "controlling for the other predictors" (cite: Cohen et al.). For example the equation for $b_1$ is

\begin{equation}
b_1 = \frac{r_{Y1} - r_{Y2} \color{red}{r_{12}}} {1 - \color{red}{r^2_{12}}} \frac{s_1}{s_Y} 
\end{equation} 

The correlation between the predictors is shown in red. Note that if the predictors are uncorrelated (i.e., $\color{red}{r^2_{12}} = 0$) then

\[ b_1 = r_{Y1} \frac{s_1}{s_Y}, \]

which is just the same equation as for simple regression. So, intuitively, the reason the formulas for the regression coefficients in the two-predictor model are more complicated than for simple regression is because they are "controlling for" or "accounting for" the relationship between the predictors. however, this is a bit of hand-wavy argument. It can be made more rigorous, and we will discuss how in class if time permits. 

### The ECLS example

Below the output from the ECLS example is reported again.**Please provide a written description what the regression coefficients for SES and ATL mean, using the interpretations above as well as any other interpretation(s) you want to talk about. If you have questions about these interpretations or their limitations, also note them now. And, please be prepared to share your thoughts in class!** 

```{r}
summary(mod1)
```

Note that we will address the rest of this R output in the next few sections, but the overall interpretation is basically the same as for simple regression.  

<!--- 
### Other interpretations 

There are other interpretation of the regression coefficients, and if time permits we will address them in class (e.g., the correlations among residuals from different models). Also, we can see in the equation for $\widehat Y$ above that the interpretation of the regression intercept is basically the same as for simple regression: it is the value of $\widehat Y$ when $X_1 = 0$ and $X_2 = 0$ (i.e., still not very interesting). 
-->

## Standardized coefficients

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

One question that arises in the interpretation of the example is the relative contribution of the two predictors to Kindergartener's Math Achievement. In particular, the regression coefficient for ALT is 10 times larger than the regression coefficient for SES -- does this mean that ALT is 10 times more important for Math Achievement that SES? 

The short answer is, "no." ALT is on a scale of 1-4 whereas SES ranges over a much larger set of values. In order to make the regression coefficients more comparable, we can standardize the $X$ variables so that they have the same variance. It is usual to go a step further and standardize all of the variables $Y, X_1, X_2$ to be z-scores with M = 0 and SD = 1. This resulting regression coefficients are called $\beta$-coefficients. For the ECLS example, these are: 

```{r}
# Unlike other software, R doesn't have a convenience functions for beta coefficients. 
z_example_data <- as.data.frame(scale(example_data))
z_mod <- lm(Math ~ SES  + ATL, data = z_example_data)
summary(z_mod)
```
Note that while the regression coefficients (and their standard errors) have changed compared to the unstandardized output reported in Section \@ref(interpretation-4), the rest of the output is the same. **Please write down an interpretation of the of beta coefficients form the above output. Note that your interpretations should include reference to the fact that the variables have been standardized. Please be prepared to share your interpretations in class!** 

There are a number of potential pitfalls of using Beta coefficients to "ease" the interpretation of the model. In the context of our example, we might wonder whether the overall cost of raising a child's Approaches to Learning by 1 SD is comparable to the overall cost of raising their family's SES by 1 SD. In general, putting variables on the same scale is only a superficial way of making comparisons among them.   

## R-squared (again) 

* What is multiple R
* R-squared with two predictors
  * venn diagram
* Adjusted R-squared [Or move this to chapter 2??]
* Do we need R-squared change yet or save until mode building? 

## Inference {#inference-4}

### Coefficients 

* show and interpret new standard formula

### R-squared

* Show and interpret new F-test
 
## Exercises 

* How much does ATL improve the prediction / explanation of Math Achievement, after considering SES? 

* How well do ATL and SES jointly predict / explain Math Achievement? 
