[["index.html", "EDUC 784: Regression Chapter 1 About This Book", " EDUC 784: Regression Peter Halpin 2022-01-14 Chapter 1 About This Book This “ebook” provides the course notes for EDUC 784. It is currently under development, so any feedback is appreciated (e.g., during class, via email, or the edit link in the header). This first chapter is just about how to use the book – the course content starts in Chapter @ref(chapter_2). "],["1.1-why-this-book.html", "1.1 Why this book?", " 1.1 Why this book? There are a few goals of moving from “textbook + slides + exercises” to an ebook. To integrate course content (slides, readings, code, examples, and exercises) into one format, rather than having multiple files to sort through on Sakai. To address the perennial problem of choosing a textbook for this course – rather than having a required text, the goal is for this ebook to become the official course text. For supplementary texts, see the course syllabus. Most importantly, having a course text that is tightly aligned with the course content means that I can be more liberal in assigning readings as homework before class, so we can spend less time in lecture and more time discussing any questions you have about the readings, going through the examples in R together, and working on assignments. As a bonus, this book is another example of cool things you can do with R. It’s written in R (https://bookdown.org) – that is crazy, right?? "],["1.2-how-to-use-this-book.html", "1.2 How to use this book", " 1.2 How to use this book The book combines lesson slides (Powerpoint / PDF) and R coding exercises (Rmarkdown / HTML) familiar from EDUC 710. You have already seen that the chapter sections of this book are quite short, closer to “slide sized” than “book-section sized.” This is so that they can double as course slides. The main trick for incorporating R exercises is called “code folding.” An example of code folding is given on this page. Below, a histogram integrated into the text. By clicking on the button called “Show Code” on the top of the page, the R code that produced the histogram will also be visible. Notice that you may need to scroll horizontally to see all of the text in the code window. Also notice that when you hover your mouse over the code window, an icon appears in the top right corner – this lets you copy the block of code with one click. # Here is some R code. You don&#39;t have to look at it when reading the book, but it is here when you need it x &lt;- rnorm(200) hist(x, col = &quot;#4B9CD3&quot;) In summary, the basic workflow is as follows. Before class, go through the assigned readings for conceptual understanding. You can skip all the code during your first reading. We will go through the assigned readings again in class together, this time focusing on any questions you have and on doing R exercises. Alright, let’s get to it! "],["2-chapter_2.html", "Chapter 2 Simple Regression", " Chapter 2 Simple Regression The focus of this course is linear regression with multiple predictors (AKA multiple regression), but we start by reviewing regression with one predictor (AKA simple regression). "],["2.1-example-2.html", "2.1 An example from NELS", " 2.1 An example from NELS Figure 2.1 shows the relationship between Grade 8 Reading Achievement (percent correct on a reading test) and Socioeconomic Status (SES; a composite measure on a scale from 0-35). The data are a subsample of the 1988 National Educational Longitudinal Survey (NELS; see https://nces.ed.gov/surveys/nels88/). # Load and attach the NELS88 data load(&quot;NELS.RData&quot;) attach(NELS) # Scatter plot plot(x = ses, y = achmat08, col = &quot;#4B9CD3&quot;, ylab = &quot;Reading Achievement (Grade 8)&quot;, xlab = &quot;SES&quot;) # Run the regression model mod &lt;- lm(achmat08 ~ ses) # Add the regression line to the plot abline(mod) Figure 2.1: Reading Achievement and SES (NELS88). The strength and direction of the linear relationship between the two variables is summarized by their correlation (specifically, the Pearson product moment correlation). In this sample, the correlation is options(digits = 2) cor(achmat08, ses) ## [1] 0.32 This is a moderate, positive correlation between Reading Achievement and SES. This correlation means that eighth graders from more well-off families (higher SES) also tended to do better in reading (higher Reading Achievement). This relationship has been widely documented and discussed in education research (e.g., https://www.apa.org/pi/ses/resources/publications/education). Please share your thoughts on this relationship in class. "],["2.2-regression-line.html", "2.2 The regression line", " 2.2 The regression line The line in the Figure 2.1 can be represented mathematically as \\[ \\hat Y = a + b X \\tag{2.1} \\] where \\(Y\\) denotes Reading Achievement (the Y-value of the blue dots) \\(X\\) denotes SES (the X-value of the blue dots) \\(\\hat Y\\) represented the values of \\(Y\\) on the line \\(a\\) represents the regression intercept (the value of \\(\\hat Y\\) when \\(X = 0\\)) \\(b\\) represents the regression slope (how much \\(\\hat Y\\) increases for each unit of increase in \\(X\\)) Note that \\(Y\\) is used to represent the actual data whereas \\(\\hat Y\\) represents the points on the line. The difference \\(e = Y - \\hat Y\\) is called a residual. The residuals for a subset of the data points in Figure 2.1 are shown in pink in Figure 2.2 # Get predicted values from regression model yhat &lt;- mod$fitted.values # select a subset of the data set.seed(10) index &lt;- sample.int(500, 30) # plot again plot(x = ses[index], y = achmat08[index], ylab = &quot;Reading Achievement (Grade 8)&quot;, xlab = &quot;SES&quot;) abline(mod) # Add pink lines segments(x0 = ses[index], y0 = yhat[index] , x1 = ses[index], y1 = achmat08[index], col = 6, lty = 2) # Overwrite dots to make it look at bit better points(x = ses[index], y = achmat08[index], col = &quot;#4B9CD3&quot;, pch = 16) Figure 2.2: Residuals for a Subsample of the Example. Notice that when we add the residuals to \\(\\hat Y\\) we get back the value of \\(Y\\): \\[\\begin{align} Y &amp; = \\hat Y + e \\\\ &amp; = a + bX + e \\tag{2.2} \\end{align}\\] We can use either format, Equation (2.1) or Equation (2.2), to write out a regression model. "],["2.3-ols.html", "2.3 OLS", " 2.3 OLS Intuitively, one approach to “fitting a line to the data” is to select the parameters of the line (its slope and intercept) to minimize the residuals. In ordinary least squares (OLS) regression, we minimize a related quantity, the sum of squared residuals: \\[ \\begin{align} SS_{\\text{res}} &amp; = \\sum_{i=1}^{N} e_i^2 \\\\ &amp; = \\sum_{i=1}^{N} (Y_i - a - b X_i)^2 \\end{align} \\] where \\(i = 1 \\dots N\\) indexes the respondents in the sample. OLS regression is very widely used and is the main focus of this course, although we will visit some other approaches (notably logistic regression) in the second half of the course. Solving the minimization problem (i.e., doing the calculus) gives the following equations for the regression parameters \\[ a = \\bar Y - b \\bar X \\quad \\quad \\quad \\quad b = \\frac{\\text{Cov}(X, Y)}{s^2_X} = r_{XY} \\frac{s_X}{s_Y} \\] (If you aren’t familiar with the symbols in these equations, check out the review materials in Section ?? for a refresher.) For the NELS example, the regression intercept and slope are, respectively: coef(mod) ## (Intercept) ses ## 48.68 0.43 Please provide an interpretation of these numbers in terms of the line in Figure 2.1, and be prepared to share your answers in class! 2.3.1 Correlation and regression Note that if \\(X\\) and \\(Y\\) are transformed to z-scores (i.e., to have mean of zero and variance of one), then \\(a = 0\\) \\(b = \\text{Cov}(X, Y) = r_{XY}\\) So, regression, correlation, and covariance are all very closely related in when we only consider two variables at a time. This is why we didn’t make a big deal about simple regression in EDUC 710. But when we get to multiple regression (i.e., more than one \\(X\\) variable), we will see that relationship between regression and correlation (and covariance) gets more complicated. "],["2.4-r-squared.html", "2.4 R-squared", " 2.4 R-squared In the previous section we saw that the predicted value of Educational Achievement increased by .43 units (about half a percentage point) for each unit of increase in SES. Another way to interpret this relationship is in terms of the proportion of variance in Reading Achievement that is associated with SES – i.e., to what extent are individual differences in Reading Achievement explained in terms of students’ SES? This question is represented graphically in Figure 2.3. The horizontal line denotes the mean of Reading Achievement. The difference between the indicated student’s Reading Achievement score and the mean can be divided into two parts. The black part shows how much closer we get to the student’s score by considering \\(\\hat Y\\) instead of \\(\\bar Y\\). This represents the extent to which Reading Achievement is explained by its linear relationship with SES. The pink part is the regression residual, which was introduced in Section 2.2. This is the variation in Reading Achievement that is “left over” after considering its linear relationship with SES. Figure 2.3: The Idea Behind R-squared. The R-squared statistic summarizes the variation in Reading Achievement associated with SES (i.e., the black part) relative to the total variation in Reading Achievement (i.e., black + pink) for all students in the sample. Aside from the regression parameters, it is the most widely used statistic in regression analysis, so we will be seeing it a lot. Some authors call it the “coefficient of determination” instead of R-squared. Using all of the cases from the example (Figure 2.1), the R-squared statistic is: options(digits = 5) summary(mod)$r.squared ## [1] 0.10128 Please write down an interpretation of this number and be prepared to share you answer in class! 2.4.1 Derivation* To derive the R-squared statistic we work the numerator of the variance, which is called the total sum of squares. \\[SS_{\\text{total}} = \\sum_{i = 1}^N (Y - \\bar Y)^2. \\] It can be re-written using the predicted values \\(\\hat Y\\): \\[SS_{\\text{total}} = \\sum_{i = 1}^N [(Y - \\hat Y) + (\\hat Y - \\bar Y)]^2. \\] The right hand side can be reduced to two other sums of squares using the rules of summation algebra (see the review in Section ??): \\[\\begin{align} SS_{\\text{total}} &amp; = \\sum_{i = 1}^N (Y - \\hat Y)^2 + \\sum_{i = 1}^N (\\hat Y - \\bar Y)^2 \\\\ \\end{align}\\] The first part is just \\(SS_\\text{res}\\) from Section 2.3. The second part is called the regression sum of squares and denoted \\(SS_\\text{reg}\\). Using this terminology we can re-write the above equation as \\[ SS_{\\text{total}} = SS_\\text{res} + SS_\\text{reg} \\] The R-squared statistic is \\[R^2 = SS_{\\text{reg}} / SS_{\\text{total}}. \\] As discussed above, this is interpreted as the proportion of variance in \\(Y\\) that is explained by its linear relationship with \\(X\\). "],["2.5-population-model.html", "2.5 The population model", " 2.5 The population model In the NELS example, the population of interest is U.S. eighth graders in 1988. We want to be able to draw conclusions about that population based on the sample of eighth graders that participated in NELS. In order to do that, we make some statistical assumptions about the population, which are collectively referred to as the population model. We talk about how to check the plausibility of these assumptions in Chapter ??. The regression population model has the following three assumptions, which are also depicted in the diagram below. Recall that the notation \\(Y \\sim N(\\mu, \\sigma)\\) means that a variable \\(Y\\) has a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Normality: The distribution of \\(Y\\) conditional on \\(X\\) is normal for all values of \\(X\\). \\[Y | X \\sim N(\\mu_{Y | X} , \\sigma_{Y | X}) \\] Homoskedasticity: The conditional distributions have equal variances (also called homegeneity of variance). \\[ \\sigma_{Y| X} = \\sigma \\] Linearity: The means of the conditional distributions are a linear function of \\(X\\). \\[ \\mu_{Y| Χ} = a + bX \\] (#fig:pop_model)The Regression Population Model. These three assumptions are summarized by writing \\[ Y|X \\sim N(a + bX, \\sigma). \\] Sometimes it will be easier to state the assumptions in terms of the population residuals, \\(\\epsilon = Y - \\mu_{Y|X}\\), which subtract off the regression line: \\(\\epsilon \\sim N(0, \\sigma)\\). An additional assumption is usually made about the data in the sample – that they are a simple random sample from the population. We will see some ways of dealing with other types of samples later on the course (e.g., Chapter ??). "],["2.6-clarifying-notation.html", "2.6 Clarifying notation", " 2.6 Clarifying notation At this point we have used the mathematical symbols for regression (e.g., \\(a\\), \\(b\\)) in two different ways: in Section 2.2 they denoted sample statistics in Section 2.5 they denoted population parameters The population versus sample notation for regression is a bit of a hot mess, but the following conventions are widely used. Concept Sample statistic Population parameter regression line \\(\\hat Y\\) \\(\\mu_{Y|X} \\text{ or } E( Y\\mid X)\\) slope \\(\\hat b\\) \\(b\\) intercept \\(\\hat a\\) \\(a\\) residual \\(e\\) \\(\\epsilon\\) variance explained \\(\\hat R^2\\) \\(R^2\\) The “hats” always denote sample quantities, and the Greek letters always denote population quantities, but there is some lack of consistency. For example, why not use \\(\\beta\\) instead of \\(b\\) for the population slope? Well, \\(\\beta\\) is conventionally used to denote standardized regression coefficients in the sample, so its already taken (more on this in the Chapter ?? ). One thing to note is that the hats are usually omitted from the statistics \\(\\hat a\\), \\(\\hat b\\), and \\(\\hat R^2\\) if it is clear from context that we are talking about the sample rather than the population. This doesn’t apply to \\(\\hat Y\\), because the hat is required to distinguish the predicted values from the data points. Another thing to note is that while \\(\\hat Y\\) is often called the predicted value(s), \\(\\mu_{Y|X}\\) is not usually referred to this way. It is called the conditional mean function or the conditional expectation function. Finally, recall that the symbol \\(E(\\cdot)\\) denotes the expected value of whatever statistic appears in the place of the dot. The expected value is a way of referring to the mean of the sampling distribution of the statistic (otherwise we would have to say things like “the mean of the mean,” which would be horrible). So \\(E(Y\\mid X)\\) is the expected value of \\(Y\\) for a given value of \\(X\\). This is equal to the population conditional mean, \\(\\mu_{Y|X}\\), when the population model is true. So the two symbols are used interchangeably. "],["2.7-inference-for-slope.html", "2.7 Inference for the slope", " 2.7 Inference for the slope When the population model is true, \\(\\hat b\\) is an unbiased estimate of \\(b\\) (i.e., \\(E(\\hat b) = b\\)). We also know the standard error of \\(\\hat b\\), which is equal to (cite) \\[ s_{\\hat b} = \\frac{s_Y}{s_X} \\sqrt{\\frac{1-R^2}{N-2}} . \\] Using these two results, we can compute t-tests and associated confidence intervals for the regression slope in the usual way. These are summarized below. See the review in Section ?? for background on these procedures. 2.7.1 t-tests The null hypothesis \\(H_0: \\hat b = b_0\\) tested against the alternative \\(H_A: \\hat b \\neq b_0\\) using the test statistic: \\[ t = \\frac{\\hat b - b_0}{s_{\\hat b}} \\] which has a t-distribution on \\(N-2\\) degrees of freedom when the null hypotesis is true. The test assumes that the population model is correct (and the sample is SRS). The null hypothesis value is usually chosen to be \\(b_0 = 0\\), in which case the test is interpreted in terms of whether the regression slope is “statistically significant.” 2.7.2 Confidence intervals For a given Type I Error rate \\(\\alpha\\), the corresponding \\((1-\\alpha) \\times 100\\%\\) confidence interval is \\[ b_0 = \\hat b \\pm t_{(1-\\alpha/2)} \\times s_{\\hat b} \\] where \\(t_{(1-\\alpha/2)}\\) denotes the \\(1 - \\alpha/2\\) quantile of the \\(t\\)-distribution with \\(N-2\\) degrees of freedom. For example, if \\(\\alpha\\) is chosen to be \\(.05\\), the corresponding \\(95\\%\\) confidence interval uses \\(t_{(.025)}\\), the 2.5-th percentile of the t-distribution. 2.7.3 The NELS example For the NELS example, the relevant information is shown in the second row of the table below (we cover the rest of the output in the next few sections): summary(mod) ## ## Call: ## lm(formula = achmat08 ~ ses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.600 -6.552 -0.148 6.023 27.663 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 48.6780 1.1282 43.15 &lt; 2e-16 *** ## ses 0.4293 0.0573 7.49 3.1e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.86 on 498 degrees of freedom ## Multiple R-squared: 0.101, Adjusted R-squared: 0.0995 ## F-statistic: 56.1 on 1 and 498 DF, p-value: 3.13e-13 The corresponding \\(95\\%\\) confidence interval is confint(mod) ## 2.5 % 97.5 % ## (Intercept) 46.46146 50.89461 ## ses 0.31668 0.54184 Please write down an interpretation of the t-test and confidence interval of the regression slope, and be prepared to share your answers in class! "],["2.8-inference-for-the-intercept.html", "2.8 Inference for the intercept", " 2.8 Inference for the intercept The situation for the regression intercept is similar to that for the slope: the OLS estimate is unbiased and its standard error is (cite) \\[ s_{\\hat a} = \\sqrt{\\frac{SS_{\\text{res}}}{N-2} \\left(\\frac{1}{N} + \\frac{\\bar X^2}{(N-1)s^2_X}\\right)}. \\] The t-tests and confidence intervals the same as for the slope, with \\(a\\) replacing \\(b\\) in the notation of the previous slide. The t-distribution also has \\(N-2\\) degrees of freedom for the intercept. It is uncommon to test the regression intercept for statistical significance in simple regression. Recall that the intercept is the value of \\(\\hat Y\\) when \\(X = 0\\). So, unless you have a hypothesis or research question about this particular value of \\(X\\) (e.g., eighth graders with \\(SES = 0\\)) there isn’t much rationale for testing the regression intercept. When we get to multiple regression, we will see some examples of regression models where the intercept is meaningful, especially when we talk about categorical predictors in Chapter ?? and interactions in Chapter ??. But, for now, we can put it on the back burner. For sake of completeness, please take a look at the R output in the previous section and provide an interpretation of the t-test and confidence interval of the regression intercept, and prepared to share your answers in class! "],["2.9-inference-for-rsquared.html", "2.9 Inference for R-squared", " 2.9 Inference for R-squared Inference for R-squared is a quite a bit different than for the regression parameters. Firstly, R-squared is a ratio of two sums of squares. We know from our study of ANOVA last semester that ratios of sums of squares are tested using an F-test, rather than a t-test. The F-test for (the population) R-squared is summarized below. 2.9.1 F-tests The null hypothesis \\(H_0: R^2 = 0\\) can be tested against the alternative \\(H_A: R^2 \\neq 0\\) using the test statistic: \\[ F = (N-2) \\frac{R^2}{1-R^2} \\] which has a F-distribution on \\(1\\) and \\(N – 2\\) degrees of freedom when the null is true. The test assumes that the population model is true. Confidence intervals for R-square are generally not reported. The R output from Section 2.7 is presented again below. Please write down an interpretation of the F-test of R-squared and be prepared to share your answers in class! summary(mod) ## ## Call: ## lm(formula = achmat08 ~ ses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.600 -6.552 -0.148 6.023 27.663 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 48.6780 1.1282 43.15 &lt; 2e-16 *** ## ses 0.4293 0.0573 7.49 3.1e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.86 on 498 degrees of freedom ## Multiple R-squared: 0.101, Adjusted R-squared: 0.0995 ## F-statistic: 56.1 on 1 and 498 DF, p-value: 3.13e-13 "],["2.10-power-analysis.html", "2.10 Power analysis", " 2.10 Power analysis Statistical power is the probability of rejecting the null hypothesis, when it is indeed false. Rejecting the null hypothesis when it is false is sometimes called a “true positive,” meaning we have correctly inferred that a parameter of interested is not zero. Power analysis is useful for designing studies so that the statistical power / true positive rate is satisfactory (usually .8 or higher). In practice, this comes down to having a large enough sample size. Power analysis in regression is very similar to power analysis for the test we studied last semester. There are four ingredients that go into a power analysis: The desired Type I Error rate, denoted \\(\\alpha\\). The desired level of statistical power. The sample size, \\(N\\). The effect size, which for regression is Cohen’s f-squared statistic (AKA the signal to noise ratio): \\[ f^2 = {\\frac{R^2}{1-R^2}}. \\] In principal, we can specify any three of these ingredients and then solve for the fourth. But, as mentioned, power analysis is most useful when we solve for \\(N\\) while planning a study. When solving for \\(N\\) “prospectively,” the effect size \\(f\\) should be based on reports of R-squared in past research. Power and \\(\\alpha\\) are usually chosen to be .8 and .05, respectively. When doing secondary data analysis (as in this class) we can instead solve for the effect size, using the sample size of the data set as our third ingredient. In the NELS example we have \\(N=500\\) observations. The output below reports the smallest effect size we can detect with a power of .8 and \\(alpha = .05\\). This is sometimes called the “minimum detectable effect size” (MDES). Note that \\(u\\) and \\(v\\) denote the degrees of freedom in the numerator and denominator of the F-test of R-squared, respectively. library(pwr) pwr.f2.test(u = 1, v = 498, sig.level = .05, power = .8) ## ## Multiple regression power calculation ## ## u = 1 ## v = 498 ## f2 = 0.015754 ## sig.level = 0.05 ## power = 0.8 Please write down an interpretation of this power analysis, and be prepared to share your answers in class! "],["2.11-end-of-readings-for-second-class.html", "2.11 End of readings for second class!", " 2.11 End of readings for second class! "],["2.12-exercises.html", "2.12 Exercises", " 2.12 Exercises We will work through this section in class together. It is currently under construction. 2.12.1 The lm function The functionlm, short for “linear model,” can estimate linear regressions using OLS and provide a lot of useful output. The main argument that the user provides to the lm function is a formula. For the simple regression of Y on X, a formula has the syntax Y ~ X Here Y denotes the outcome variable, the tilde ~ roughly means “equals,” and X is the predictor variable. We will see more complicated formulas as we go through the course. For more information on R’s formula syntax, see help(formula). Let’s take a closer look using the following two variables from the NELS data (see Sakai resource folder for more information on the data). achmat08: eighth grade math achievement (percent correct on a math test) ses: a composite measure of socio-economic status, on a scale from 0-35 # Load and attach the data -- see last week&#39;s exercises for details #load(&quot;NELS.RData&quot;) #attach(NELS) plot(x = ses, y = achmat08, col = &quot;#4B9CD3&quot;) # Regress math achievement on SES mod &lt;- lm(achmat08 ~ ses) # Print out the regression coefficients coef(mod) ## (Intercept) ses ## 48.67803 0.42926 Let’s do some quick calculations to check that the lm output corresponds the formulas for the slope and intercept we used in the lesson: \\[ a = \\bar Y - b \\bar X \\quad \\text{and} \\quad b = \\frac{\\text{Cov}(X, Y)}{s_X^2} \\] # Confirm that the slope from m is just the covariance divided by the variance of X cov_xy &lt;- cov(achmat08, ses) s_x &lt;- var(ses) b &lt;- cov_xy / s_x b ## [1] 0.42926 # Confirm that the y-intercept is obtained from the two means and the slope xbar &lt;- mean(ses) ybar &lt;- mean(achmat08) a &lt;- ybar - b * xbar a ## [1] 48.678 Let’s also check our interpretation of the parameters. If the answers to these questions are not clear, please make sure to ask in class or on the Sakai forum. What is the value of achmat08 when ses is equal to zero? How much do the predicted values of achmat08 increase for each unit of increase in ses? 2.12.2 Predicted values and residuals The lm function also returns the residuals \\(e_i\\) and the predicted values \\(\\widehat{Y_i}\\), which we can access using the $ operator.These will be useful later on for computing R-squared. For now we will just plot the two values to show that the predicted values and the residuals are uncorrelated, which was discussed in class. yhat &lt;- mod$fitted.values res &lt;- mod$resid # In OLS, the predicted values and the residuals are uncorrelated: plot(yhat, res, col = &quot;#4B9CD3&quot;) cor(yhat, res) ## [1] -2.4274e-16 2.12.3 Variance explained Above we found out that the regression coefficient was 0.4-ish. Is that good? or average? or what? One way to answer that question is by considering the amount of variation in \\(Y\\) that is associated with (or explained by) its relationship with \\(X\\). Recall from our lesson that one way to do this is via the variance decomposition \\[ SS_{total} = SS_{res} + SS_{reg}\\] from which we can compute the proportion of variation in Y that is associated with the regression model \\[R^2 = \\frac{SS_{reg}}{SS_{total}}\\] Let’s compute \\(R^2\\) for our example. # Compute the sums of squares ybar &lt;- mean(achmat08) ss_total &lt;- sum((achmat08 - ybar)^2) ss_reg &lt;- sum((yhat - ybar)^2) ss_res &lt;- sum((achmat08 - yhat)^2) # Check that SS_total = SS_reg + SS_res ss_total ## [1] 43527 ss_reg + ss_res ## [1] 43527 # Compute R-squared ss_reg/ss_total ## [1] 0.10128 # Check that R-square is really equal to the square of the PPMC # Note -- this is only true for simple regression ppmc &lt;- cor(achmat08, ses) ppmc^2 ## [1] 0.10128 2.12.4 Inference At this point we can say that SES explained about 10% of the variation in eighth grade students’ math achievement, in our sample. However, we haven’t yet talked aboutstatistical inference, or how we can make conclusions about a population based on a sample from that population. Let’s use the summary function to test the coefficients in our model. mod &lt;- lm(achmat08 ~ ses) summary(mod) ## ## Call: ## lm(formula = achmat08 ~ ses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.600 -6.552 -0.148 6.023 27.663 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 48.6780 1.1282 43.15 &lt; 2e-16 *** ## ses 0.4293 0.0573 7.49 3.1e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.86 on 498 degrees of freedom ## Multiple R-squared: 0.101, Adjusted R-squared: 0.0995 ## F-statistic: 56.1 on 1 and 498 DF, p-value: 3.13e-13 In the table, the t-test and p-values are for the null hypothesis that the corresponding coefficient is zero in the population. We can see that the intercept and slope are both significantly different from zero. The text below the table summarizes the output for R-square, including the F-test, it’s degrees of freedom, and the p-value. (We will talk about adjusted R-square later on.) We can use the confint function to obtain confidence intervals for the regression coefficients. Use help to find out more about the confint function. confint(mod) ## 2.5 % 97.5 % ## (Intercept) 46.46146 50.89461 ## ses 0.31668 0.54184 Be sure to remember the correct interpretation of confidence intervals: there is a 95% chance that the interval includes the true parameter value (not: there is a 95% chance that the parameter falls in the interval). For example, there is a 95% chance that the interval [.31, .54] includes the true regression coefficient for SES. 2.12.5 Power analysis Power analyses should ideally be done prospectively – i.e., before the data are collected. Since this class will work with secondary data analyses, most of our analyses will be retrospective. But don’t let this mislead you about the importance of statistical power – you should always do a power analysis before collecting data!! To do a power analsyes in R, we can install and load the pwr package. If you haven’t installed an R package before it’s pretty straight forward – but just ask course staff or a fellow student if you run into any issues. # Install the package # Note you may have to select a mirror -- I suggest using Kansas (KS) install.packages(&quot;pwr&quot;) # Load the package by using the library command library(&quot;pwr&quot;) # Use the help menu to see what the package does help(&quot;pwr-package&quot;) To do a power analysis for linear regression, it is common to use Cohen’s \\(f^2\\) as the effect size: \\[f^2 = \\frac{R^2}{1-R^2}.\\] Recall that \\(R^2\\) is the proportion of variance in \\(Y\\) explained by the model, and so \\(1 - R^2\\) is the proportion of variance not explained by the model. Thus, \\(f^2\\) can be interpreted as a signal to noise ratio. In addition to the effect size, we need to know the degrees of freedom for the F-test of R-square. The pwr functions use the following notation: u is the degrees of freedom in the numerator of an F-test. v is the degrees of freedom in the denominator of an F-test. In simple regression, u = 1 and v = N - 2. 2.12.6 Example: A prospective power analysis How many observations would be required to detect an effet size of R-square = .1, using \\(\\alpha = .05\\) and power = .8? To find the answer, enter the provided information into the pwr.f2.test function, and the function will solve for the “missing piece” – in this case \\(v = N - 2\\). # Use the provided values of R2, alpha, power (and u = 1) to solve for v = N - 2 R2 &lt;- .1 f2 &lt;- R2/(1-R2) pwr.f2.test(u = 1, f2 = f2, sig.level = .05, power = .8) ## ## Multiple regression power calculation ## ## u = 1 ## v = 70.611 ## f2 = 0.11111 ## sig.level = 0.05 ## power = 0.8 In this example we find that \\(v = 70.6\\). Since \\(v = N - 2\\), so we know that a sample size of \\(N = 72.6\\) (rounded up to 73) is required to reject the null hypothesis that \\(R^2 = 0\\), when the true popuation value is \\(R^2 = .1\\), with a power of .8 and using a significance level of .05. "],["2.13-review-material.html", "2.13 Review material", " 2.13 Review material Under construction… "],["3-chapter_3.html", "Chapter 3 Interpretations of regression", " Chapter 3 Interpretations of regression Before moving onto more complicated regression models, let’s consider why we might be interested in them first place. As discussed in the following sections, regression has three main uses: Prediction (focus on \\(\\hat Y\\)) Causation (focus on \\(b\\)) Explanation (focus on \\(R^2\\)) "],["3.1-regression-and-prediction.html", "3.1 Regression and prediction", " 3.1 Regression and prediction Prediction (etymology: “to make known beforehand”) means that we want to use \\(X\\) to make a guess about \\(Y\\). This use of regression makes the most sense when we know the value of \\(X\\) before we know the value of \\(Y\\). When we are interested in using values of \\(X\\) to make predictions about (yet unobserved) values of \\(Y\\), we use \\(\\hat Y\\) as our guess. This is why \\(\\hat Y\\) is called the “predicted value” of \\(Y\\). When making predictions, we usually want some additional information about how good the predictions will be. In OLS regression, this information is provided by the prediction error variance (cite: Fox) \\[ s^2_{\\hat Y_i} = \\frac{SS_{\\text{res}}}{N - 2} \\left( \\frac{1}{N} + \\frac{(X_i - \\bar X)^2}{(N-1) s^2_X} \\right). \\] The prediction errors for the data in Figure 2.2 are depicted in Figure 3.1 as a gray band around the regression line. # Using a different plotting library that adds prediction error bands (need to double check computation) library(ggplot2) ggplot(NELS[index, ], aes(x = ses, y = achmat08)) + geom_point(color=&#39;#3B9CD3&#39;, size = 2) + geom_smooth(method = lm, color = &quot;grey35&quot;) + ylab(&quot;Reading Achievement (Grade 8)&quot;) + xlab(&quot;SES&quot;) + theme_bw() Figure 3.1: Prediction Error for Example Data. Notice that the prediction error variance increases with \\(SS_{\\text{res}}\\) – in other words, the larger the residuals (Figure 2.2), the worse the prediction error. As we will see in this Section (??), one way to reduce \\(SS_{\\text{res}}\\) is to add more predictors into the model – i.e., multiple regression. 3.1.1 More about Prediction Prediction was the original use of regression (see https://en.wikipedia.org/wiki/Regression_toward_the_mean#History). More recent methods developed in machine learning also focus mainly on prediction – although the methods used in machine learning are often more complicated than OLS regression, and the research context is usually quite different, the basic problem is the same. Machine learning has led to the use of out of sample predictions, rather than prediction error, as the main criterion for judging the quality of predictions made from a model. Machine learning has also introduced some new techniques for choosing which potential predictors to include in a model (i.e., “variable selection”). We will touch on these topics later in the course, although our main focus is OLS regression. "],["3.2-regression-and-causation.html", "3.2 Regression and causation", " 3.2 Regression and causation A causal interpretation of regression means that that changing \\(X\\) by one unit will change \\(\\mu_{Y|X}\\) by \\(b\\) units. Note that this is an assumption about the population model, specifically the conditional expectation function. This is a much stronger interpretation than prediction because it requires stronger assumptions. In particular, regression parameters can only be interpreted causally when all variables that are correlated with \\(Y\\) and \\(X\\) are included as predictors in the model. When a variable is left out, this is called omitted variable bias. This situation is nicely explained by Gelman and Hill (cite: Gelman), and a modified version of their discussion is provided below. This discussion is a bit technical, but the take-home message is summarized in the following points When a predictor variable that is correlated with \\(Y\\) and \\(X\\) is left out of a regression model, this is called omitted variable bias. The overall idea is basically the same as saying “correlation does not imply causation” or the notion of spurious correlations. This is also an example of what is called “endogeneity” in regression (etymology: originating from within). In order to avoid omitted variable bias, we want to include more than one predictor in our regression models – i.e., multiple regression. 3.2.1 Omitted variable bias* We start by assuming a “true” regression model with two predictors. In the context of our example, this means that there is one other variable, in addition to SES, that is important for predicting Reading Achievement. Of course, there are many predictors of Reading Achievement (see Section ??), but we only need two to explain the problem of omitted variable bias. Let’s write the “true” model as: \\[\\begin{equation} \\hat Y = a + b_1 X_1 + b_2 X_2 \\tag{3.1} \\end{equation}\\] where \\(X_1\\) is SES and \\(X_2\\) is any other variable that is correlated with both \\(Y\\) and \\(X_1\\) (e.g., number of books in the household). Next, imagine that instead of using the model in (3.1), we analyze the data using the model with just SES. In our example, this would reflect a situation in which we don’t have data on the number of books in the house, so we have to make due with just SES, leading to the usual regression line (Section 2.2): \\[ \\hat Y = a^* + b^*_1 X_1. \\tag{3.2} \\] The problem of omitted variable bias is that \\(b_1 \\neq b^*_1\\) – i.e., the regression parameter in the true model is not the same as the regression parameter in our data-analytic model with only one predictor. This is perhaps surprising – leaving out the number of books in the household gives us the wrong regression parameter for SES! To see why, start by writing \\(X_2\\) as a function of \\(X_1\\). \\[ X_2 = \\alpha + \\beta X_1 + \\epsilon. \\tag{3.3} \\] (Side note: by adding the residual into the model for \\(X_2\\), we ensure the equality holds for \\(X_2\\) not just \\(\\hat X_2\\) – more on this later.) Then we use Equation (3.3) to substitute for \\(X_2\\) in Equation (3.1), \\[\\begin{align} \\hat Y &amp; = a + b_1 X_1 + b_2 X_2 \\\\ \\hat Y &amp; = a + b_1 X_1 + b_2 (\\alpha + \\beta X_1 + \\epsilon) \\\\ \\hat Y &amp; = \\color{orange}{(a + \\alpha + \\epsilon)} + \\color{green}{(b_1 + b_2\\beta)} X_1. \\tag{3.4} \\end{align}\\] Notice that in the last line of Equation (3.4), \\(Y\\) is predicted using only \\(X_1\\), so it is equivalent to Equation (3.2). Based on this comparison, we can write \\(a^* = \\color{orange}{(a + \\alpha + \\epsilon)}\\) \\(b^*_1 = \\color{green}{(b_1 + b_2\\beta)}\\) This last equation for \\(b^*_1\\) is what we are interested in. It shows that the regression parameter in our data analytic model using just SES, \\(b^*_1\\), is not equal to the “true” regression parameter using both predictors, \\(b_1\\). This is what omitted variable means – leaving out \\(X_2\\) in Equation (3.2) gives us the wrong regression parameter for \\(X_1\\). This is one of the main motivations for including more than one predictor variable in a regression model – i.e., to avoid omitted variable bias. Notice that there two special situations in which omitted variable bias is not a problem: when the two predictors are not linearly related – i.e., \\(\\beta = 0\\), or when the second predictor is not linearly related to \\(Y\\) – i.e., \\(b_2 = 0\\). We will discuss the interpretation of these situations in class. "],["3.3-regression-and-explanation.html", "3.3 Regression and explanation", " 3.3 Regression and explanation In the social sciences, many uses of regression fall somewhere between prediction and causation. We want to do more than just predict outcomes of interest, but we often don’t have a basis for making strong assumptions required for causal interpretation of regression coefficients. This gray area between prediction and causation can be referred to as explanation. In terms of our example, we might want to explain why eighth graders differ in there Reading Achievement in terms of a large number of potential predictors, such as Student factors attendance past academic performance in Reading past academic performance in other subjects (Question: why include this? Hint: see previous section) School factors their ELA teacher the school they attend their peers (e.g., the school’s catchment area) Home factors SES Number of books in the household Maternal education Even this long list leaves out potential omitted variables. But, by including more than one predictor, we can get “closer” to a causal interpretation through “statistical controls” (See Chapter 3). When the goal of an analysis is explanation, it usual to focus on the proportion of variation in the outcome variable that is explained by the predictor(s) This proportion of variance is denoted \\(R^2\\) (“R-squared”) and is the first topic we address in our next lesson … "],["4-chapter-4.html", "Chapter 4 Regression with two predictors", " Chapter 4 Regression with two predictors Compare two simple regressions with multiple regression "],["4.1-semi-partial-correlation.html", "4.1 Semi partial correlation", " 4.1 Semi partial correlation "],["references.html", "References", " References "]]
