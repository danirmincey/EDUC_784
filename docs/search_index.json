[["4-chapter-4.html", "Chapter 4 Regression with Two Predictors ", " Chapter 4 Regression with Two Predictors "],["4.1-ecls-4.html", "4.1 An example from ECLS", " 4.1 An example from ECLS This section considers a subset of data from the 1998 Early Childhood Longitudinal Study (ECLS; https://nces.ed.gov/ecls/). We focus on the following three variables. Math Achievement in the first semester of Kindergarten. This variable can be interpreted as the number of questions out of 61 answered correctly on a math test. Don’t worry – the respondents in this study did not have to write a 61-question math test in the first semester of K! Students only answered a few of the questions and their scores were re-scaled to be out of the total of 61 questions afterwards. Socioecomonic Status (SES), which is a composite of household factors (e.g., parental education, household income) ranging from 30-72. Approaches to Learning (ATL), which is a teacher reported measure of behaviors that affect the ease with which children can benefit from the learning environment. It includes six items that rate the child’s attentiveness, task persistence, eagerness to learn, learning independence, flexibility, and organization. The items have 4 response categories (1-4), coded so that higher values represent more positive responses, and the scale is an unweighted average the six items. More details about these variables are available in the ECLS user manual: https://nces.ed.gov/ecls/data/ECLSK_K8_Manual_part1.pdf. In the scatter plots below, the panels are arranged in matrix format. The variable named on the diagonal appears on the vertical (\\(Y\\)) axis in its row and the horizontal (\\(X\\)) axis in its column. For example, Math Achievement is on the vertical axis in the first row and the horizontal axis in the first column. Notice that plots below the diagonal are a mirror image of the plots above the diagonal. load(&quot;ECLS250.RData&quot;) attach(ecls) example_data &lt;- data.frame(c1rmscal, wksesl, t1learn) names(example_data) &lt;- c(&quot;Math&quot;, &quot;SES&quot;, &quot;ATL&quot;) pairs(example_data , col = &quot;#4B9CD3&quot;) Figure 4.1: ECLS Example Data. The format of Figure 4.1 is the same as that of the correlation matrix among the variables: options(digits = 2) cor(example_data) ## Math SES ATL ## Math 1.00 0.44 0.40 ## SES 0.44 1.00 0.29 ## ATL 0.40 0.29 1.00 Again, notice that the entries below the diagonal are mirrored by the entries above the diagonal. If you have questions about how these plots and correlations are presented, please write them down now and share them class. We can see that all three variables are positively correlated. SES and ATL have similar correlations with Math Achievement (.44 and .40, respectively), and are also moderately correlated with each other (.29). What do you think about these relationships in the context of the current debate about funding universal pre-K in the United States? "],["4.2-the-two-predictor-model.html", "4.2 The two-predictor model", " 4.2 The two-predictor model In the ECLS example, we can think of Kindergarteners’ Math Achievement as the outcome variable, with SES and Approaches to Learning as potential predictors / explanatory variables. The multiple regression model for this example can be written as \\[ \\widehat Y = b_0 + b_1 X_1 + b_2 X_2 \\tag{4.1} \\] where \\(\\widehat Y\\) denotes the predicted Math Achievement scores. \\(X_1 = \\;\\) SES and \\(X_2 = \\;\\) ATL (it doesn’t matter which predictor we denote as \\(1\\) or \\(2\\)). \\(b_1\\) and \\(b_2\\) are the regression slopes. The intercept is denoted by \\(b_0\\) (rather than \\(a\\)). Just like simple regression, the residual for Equation (4.1) is defined as \\(e = Y - \\widehat Y\\) and the model can be equivalently written as \\(Y = \\widehat Y + e\\). The correlations reported in Section 4.1 address how the three variables are (linearly) related in the ECLS data. Multiple regression lets us additionally address the following types of questions: Does ATL “add anything” to our understanding of Math Achievement, beyond SES alone? What is the relative importance of the two predictors? How much of the variance in Math Achievement do they explain? As a first step towards answering these questions, the next section contrasts multiple regression with simple regression. "],["4.3-comparison-4.html", "4.3 Multiple vs simple regression", " 4.3 Multiple vs simple regression At first glance, it might appear that simple regression and multiple regression are essentially the same thing. However, there is an important ingredient in multiple regression that is missing from simple regression. This section illustrates how the two approaches differ and asks you to think about what the missing ingredient might be. Table 4.1 compares the output of three regression models using the ECLS example. “Multiple” is a two-predictor model that regresses Math Achievement on SES and ATL. “Simple (SES)” regresses Math Achievement on SES only. “Simple (ALT)” regresses Math Achievement on ALT only. # Run models mod1 &lt;- lm(Math ~ SES + ATL, data = example_data) mod2a &lt;- lm(Math ~ SES, data = example_data) mod2b &lt;- lm(Math ~ ATL, data = example_data) # Collect output out1 &lt;- c(coef(mod1), summary(mod1)$r.squared) out2a &lt;- c(coef(mod2a), NA, summary(mod2a)$r.squared) out2b &lt;- c(coef(mod2b), NA, summary(mod2b)$r.squared)[c(1,3,2,4)] out &lt;- data.frame(rbind(out1, out2a, out2b)) # Clean up names names(out) &lt;- c(names(coef(mod1)), &quot;R-squared&quot;) out$Model &lt;- c(&quot;Multiple&quot;, &quot;Simple (SES)&quot;, &quot;Simple (ATL)&quot;) out &lt;- out[c(5, 1:4)] row.names(out) &lt;- NULL # Table options(knitr.kable.NA = &#39;---&#39;) knitr::kable(out, caption = &quot;Regression Coefficients and R-squared From the Three Models&quot;) Table 4.1: Regression Coefficients and R-squared From the Three Models Model (Intercept) SES ATL R-squared Multiple -6.05 0.35 3.5 0.27 Simple (SES) 0.62 0.44 — 0.19 Simple (ATL) 7.04 — 4.7 0.16 There are two main things to notice about the table: The regression coefficients in the multiple regression model do not equal the regression coefficients in the two simple regressions. This is an illustration of omitted variable bias, which we discussed in Section 3.2. The proportion of variance explained (R-squared) in the two-predictor model is less than the sum of the proportion of variance explained in the two simple models. Why is this weird? Well, we know that total variance of Math Achievement isn’t changing – i.e., \\(SS_\\text{total}\\) is the same in all of the models. Therefore the R-squared values are all fractions with the same denominator, so they should be additive (e.g., \\(a/c + b/c = (a + b)/ c\\)). But the values in the table don’t follow this pattern. In summary, the regression coefficients and R-squared in the multiple regression model are different than what we would expect by extending simple regression in a naive way (i.e., by doing simple regression multiple times). 4.3.1 What is the missing ingredient? Recall that in simple regression, the regression slope is just a repackaging of the correlation between the outcome and predictor (see Section 2.3). So, the “Simple (SES)” model considers the correlation between Math Achievement and SES, and the “Simple ATL” model considers the correlation between Math Achievement and ATL. These two models leave out one of the correlations from Section 4.1 – which one? Bonus: Explain why this constitutes a case of omitted variable bias. Please write down your answers and be prepared to share them in class! "],["4.4-ols-4.html", "4.4 OLS with two predictors", " 4.4 OLS with two predictors We can estimate the parameters of the two-predictor regression model in Equation (4.1) model using same approach as for simple regression, OLS. We do this by choosing the values of \\(b_0, b_1, b_2\\) that minimize \\[SS_\\text{res} = \\sum_i e_i^2.\\] Solving the mimmization problem leads to the following equations for the regression coefficients (the subscripts \\(j = 1, 2\\) denote \\(X_j\\)) \\[\\begin{align} b_0 &amp; = \\bar Y - b_1 \\bar X_1 - b_2 \\bar X_2 \\\\ \\\\ b_1 &amp; = \\frac{r_{Y1} - r_{Y2} r_{12}}{1 - r^2_{12}} \\frac{s_1}{s_Y} \\\\ \\\\ b_2 &amp; = \\frac{r_{Y2} - r_{Y2} r_{12}}{1 - r^2_{12}} \\frac{s_2}{s_Y} \\tag{4.2} \\end{align}\\] As promised, these equations are more complicated than for simple regression :) The next section addresses the interpretation of the regression coefficients. "],["4.5-interpretation-4.html", "4.5 Interpreting the coefficients", " 4.5 Interpreting the coefficients An important part of using multiple regression is getting the correct interpretation of the regression coefficients. The basic interpretation is that the slope parameter for SES represents how much predicted Math Achievement changes for a one unit increase of SES, while holding ATL constant. (The same interpretation holds when switching the predictors.) The important difference with simple regression is the “holding the other predictor constant” part, so let’s dig into it. 4.5.1 “Holding the other predictor constant” We can start by revisitng the regression model in Equation (4.1): \\[ \\widehat Y = b_0 + b_1 X_1 + b_2 X_2 \\] If we increase SES (\\(X_1\\)) by one unit and hold ATL (\\(X_2\\)) constant, we get \\[ \\widehat{Y^*} = b_0 + b_1 (X_1 + 1) + b_2 X_2. \\] The difference between \\(\\widehat{Y^*}\\) and \\(\\widehat{Y}\\) is how much the predicted value changes for a one unit increase in SES, while holding ATL constant: \\[ \\widehat{Y^*} - \\widehat{Y} = b_0\\] So, the interpretation of the coefficients in multiple regression as “holding the other predictor(s) constant” is an immediate consequence of the model. One draw back of this interpretation is that holding one predictor constant while changing another might not make sense in some applications. In fact, if the predictors are correlated, this means that changes in one predictor are associated with changes in the other. There following interpretation addresses this issue. 4.5.2 “Controlling for the other predictor” The equations in for \\(b_1\\) and \\(b_2\\) in Section 4.4 admit another interpretation in terms of “controlling for the other predictor” (cite:Cohen). For example, the equation for \\(b_1\\) is \\[\\begin{equation} b_1 = \\frac{r_{Y1} - r_{Y2} \\color{red}{r_{12}}} {1 - \\color{red}{r^2_{12}}} \\frac{s_1}{s_Y} \\end{equation}\\] The correlation between the predictors is shown in red. Note that if the predictors are uncorrelated (i.e., \\(\\color{red}{r^2_{12}} = 0\\)) then \\[ b_1 = r_{Y1} \\frac{s_1}{s_Y}, \\] which is just the regression coefficient from simple regression (Section 2.3). In other words, the reason the formulas for the regression coefficients in the two-predictor model are more complicated than for simple regression is because they are “controlling for” or “accounting for” the relationship between the predictors. This argument is a bit of hand-wavy. It can be made more rigorous (cite:Pehazur), and we will discuss how in class if time permits. 4.5.3 The ECLS example Below, the R output from the ECLS example is reported. Please provide a written explanation of the regression coefficients for SES and ATL, using the interpretations above and / or any other interpretations you want to talk about. If you have questions about how to interpret the coefficients, also note them now. And, please be prepared to share your thoughts in class! summary(mod1) ## ## Call: ## lm(formula = Math ~ SES + ATL, data = example_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.10 -4.03 -1.07 3.29 29.54 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.0502 2.9003 -2.09 0.038 * ## SES 0.3512 0.0563 6.24 1.9e-09 *** ## ATL 3.5212 0.6739 5.23 3.7e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.2 on 247 degrees of freedom ## Multiple R-squared: 0.273, Adjusted R-squared: 0.267 ## F-statistic: 46.3 on 2 and 247 DF, p-value: &lt;2e-16 "],["4.6-beta-4.html", "4.6 Standardized coefficients", " 4.6 Standardized coefficients One question that arises in the interpretation of the example is the relative contribution of the two predictors to Kindergartener’s Math Achievement. In particular, the regression coefficient for ALT is 10 times larger than the regression coefficient for SES – does this mean that ALT is 10 times more important than SES? The short answer is, “no.” ALT is on a scale of 1-4 whereas SES ranges over a much larger set of values. In order to make the regression coefficients more comparable, we can standardize the \\(X\\) variables so that they have the same variance. Many researchers go a step further and standardize all of the variables \\(Y, X_1, X_2\\) to be z-scores with M = 0 and SD = 1. The resulting regression coefficients are often called \\(\\beta\\)-coefficients or \\(\\beta\\)-weights. Comparison with Equations (4.2) shows that \\(\\beta_0 = 0\\) and \\[ \\beta_j = b_j \\frac{s_Y}{s_j}. \\] For the ECLS example, the \\(\\beta\\)-weights are reported below. Notice that, while the regression coefficients (and their standard errors) have changed compared to the unstandardized output reported in Section 4.5, much of the output is the same (t-tests, p-values, R-squared, its F-test). # Unlike other software, R doesn&#39;t have a convenience functions for beta coefficients. z_example_data &lt;- as.data.frame(scale(example_data)) z_mod &lt;- lm(Math ~ SES + ATL, data = z_example_data) summary(z_mod) ## ## Call: ## lm(formula = Math ~ SES + ATL, data = z_example_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.959 -0.560 -0.149 0.457 4.104 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.22e-16 5.42e-02 0.00 1 ## SES 3.53e-01 5.67e-02 6.24 1.9e-09 *** ## ATL 2.96e-01 5.67e-02 5.23 3.7e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.86 on 247 degrees of freedom ## Multiple R-squared: 0.273, Adjusted R-squared: 0.267 ## F-statistic: 46.3 on 2 and 247 DF, p-value: &lt;2e-16 Please write down an interpretation of the of beta coefficients in the above output. Your interpretation should include reference to the fact that the variables have been standardized. Please be prepared to share your interpretation / questions in class! There are a number of potential pitfalls of using Beta coefficients to “ease” the comparison of regression coefficients. In the context of our example, we might wonder whether the overall cost of raising a child’s Approaches to Learning by 1 SD is comparable to the overall cost of raising their family’s SES by 1 SD. In general, putting variables on the same scale is only a superficial way of making comparisons among their regression coefficients. "],["4.7-rsquared-4.html", "4.7 (Multiple) R-squared", " 4.7 (Multiple) R-squared R-squared in multiple regression has the same general formula and interpretation as in simple regression: \\[ R^2 = \\frac{SS_{\\text{reg}}} {SS_{\\text{total}}}. \\] As shown in the previous sections, the R-squared for the ECLS example is equal to .273. Please write down your interpretation of this value and be prepared to share your answer in class. As discussed below, we can also say a bit more about R-squared in multiple regression. 4.7.1 The multiple correlation \\(R\\), the square-root of \\(R^2\\), is called the multiple correlation because \\[R = \\text{Cor}(Y, \\widehat Y). \\] It is the correlation between the observed \\(Y\\) values and the predicted \\(\\widehat Y\\) values. In simple regression, the multiple correlation is just the same the regular correlation coefficient \\(r_{XY}\\). But in multiple regression, it is the correlation between the observed \\(Y\\) values and a linear combination of the \\(X\\) values (i.e., \\(\\widehat Y\\)), so it gets a special name. 4.7.2 Relation with simple regression Like the regression coefficients in Equation (4.2), the equation for R-squared can also be written in terms of the correlations among the three variables: \\[ R^2 = \\frac{r^2_{Y1} + r^2_{Y2} - 2 r_{12}r_{Y1}r_{Y2}}{1 - r^2_{12}} \\] If the correlation between the predictors is zero, then we have the simplified formula \\[ R^2 = r^2_{Y1} + r^2_{Y2}. \\] When the predictors are correlated, either positively or negatively, it can be show that \\[ R^2 &lt; r^2_{Y1} + r^2_{Y2}. \\] This explains the relationship among the R-squared values in Table 4.1. The sum of the R-squared values in the simple models is only equal to the R-squared value in the two-predictor when the predictors are not correlated. Otherwise, it the sum is larger than the multiple R-squareds. 4.7.3 Adjusted R-squared The sample R-squared is an upwardly biased estimate of the population R-squared. The cause of the bias for the case of simple regression when \\(\\rho = 0\\) is illustrated in the figure below (\\(\\rho\\) is the population correlation). Figure 4.2: Sampling Distribution of \\(r\\) and \\(r^2\\) when $ ho = 0$. For the “un-squared” correlation, \\(r\\), the sample distribution is centered at the true value \\(\\rho = 0\\), so it is an unbiased estimate of \\(\\rho\\). But for the squared correlation, \\(r^2\\), the mean of the sampling distribution is slightly above zero because all of the random deviations from the population value are in the same direction (because they have been squared). So it is an upwardly biased (i.e., too large) estimate of \\(\\rho^2 = 0\\). The adjusted R-squared corrects this bias. The formula is: \\[ \\tilde R^2 = 1 - (1 - R^2) \\frac{N-1}{N - J - 1} \\] where \\(J\\) is the number of predictors in the model. It can be seen that the adjustment is larger when The number of predictors \\(J\\) is large relative to the sample size \\(N\\). R-squared is closer to zero. So, roughly speaking, the adjustment will be more severe when there are a lot of predictors in the model, but the predictors don’t explain a lot of variation in the outcome. In general, adjusted R-squared should be reported whenever it would lead to different substantive conclusions than the un-adjusted value. "],["4.8-inference-for-slopes-4.html", "4.8 Inference for the slopes", " 4.8 Inference for the slopes There isn’t really any thing new that about inference with multiple regression, except the formula for the standard errors: \\[ s_{\\widehat b_j} = \\frac{s_y}{s_x} \\sqrt{\\frac{1 - R^2}{N - J - 1}} \\times \\sqrt{\\frac{1}{1 - R_j^2}} \\tag{4.3} \\] In this formula, \\(J\\) denotes the number of predictors and \\(R^2_j\\) is the R-squared that results from regressing predictor \\(j\\) on the other \\(J-1\\) predictors (without the \\(Y\\) variable). Notice that the first part of the standard error (before the “\\(\\times\\)”) is the same as simple regression (see Section 2.7). The last part, which includes \\(R^2_j\\), is unique to multiple regression. The standard errors can be used to construct t-tests and confidence intervals using the same approach as simple regression (see Section 2.7). The degrees of freedom for the t-distribution are \\(N - J -1.\\) The R output for the ECLS example is presented (again) below. Please write down your conclusions about the statistical significance of the predictors and be prepared to share your answer in class. summary(mod1) ## ## Call: ## lm(formula = Math ~ SES + ATL, data = example_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.10 -4.03 -1.07 3.29 29.54 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.0502 2.9003 -2.09 0.038 * ## SES 0.3512 0.0563 6.24 1.9e-09 *** ## ATL 3.5212 0.6739 5.23 3.7e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.2 on 247 degrees of freedom ## Multiple R-squared: 0.273, Adjusted R-squared: 0.267 ## F-statistic: 46.3 on 2 and 247 DF, p-value: &lt;2e-16 4.8.1 Comments on precision We can use Equation (4.3) to understand the factors that influence the size of the standard errors. Recall that standard errors describe the sample-to-sample variability of a statistic. If there is a lot sample-to-sample variability, the statistic is said to be imprecise. The standard errors decrease with The sample size, \\(N\\) The proportion of variance in the outcome explained by the predictors, \\(R^2\\) The standard errors increase with The number of predictors, \\(J\\) The proportion of variance in the predictor that is explained by the other predictors, \\(R^2_j\\) So, large sample sizes and small residual variance (\\(1 - R^2\\)) lead to high precision in multiple regression. On the other hand, including many highly correlated predictors in the model leads to less precision. In particular, the situation where \\(R^2_j\\) approaches the value of \\(1\\) is called multicollinearity (or just collinearity with 2 predictors). We will talk about multicollinearity in more detail in Chapter ??. "],["4.9-inference-for-rsquared-4.html", "4.9 Inference for R-squared", " 4.9 Inference for R-squared The R-squared statistic in multiple regression tells us how much variation in the outcome is explained by all of the predictors together. If the predictors do not explain any variation, then the population R-squared is equal to zero. Notice that \\(R^2 = 0\\) implies \\(b_1 = b_2 = ... = b_J = 0\\) (in the population). So, testing the significance of R-squared is equivalent to testing whether any of the regression parameters are non-zero. When we addressed ANOVA last semester, we called this the omnibus hypothesis. But in regression analysis, it is usually just referred to as a test of R-squared. The null hypothesis \\(H_0 : R^2 = 0\\) can be tested using the statistic \\[ F = \\frac{\\widehat R^2 / J}{(1 - \\widehat R^2) / (N - J - 1)}, \\] which has an F-distribution on \\(J\\) and \\(N - J -1\\) degrees of freedom when the null hypothesis is true. Using the R output reported in the previous section, please write down your conclusion about the statistical significance of the R-squared statistic in the ECLS example. "],["4.10-workbook.html", "4.10 Workbook", " 4.10 Workbook This section collects the questions asked in this chapter. We will discuss these questions in class. If you haven’t written down / thought about the answers to these questions before class, the lesson will not be very useful for you! So, please attempt to engage with each question by writing down one or more answers, asking clarifying questions, posing follow up questions, etc. Section 4.1 If you have questions about the interpretation of a correlation matrix (below) or pairwise plots (see Section 4.1), please write them down now and share them class. cor(example_data) ## Math SES ATL ## Math 1.00 0.44 0.40 ## SES 0.44 1.00 0.29 ## ATL 0.40 0.29 1.00 Section 4.3 The two simple regression models (Math ~ SES and Math ~ ATL) leave out one of the correlations from Section 4.1 – which one? Bonus: Explain why this constitutes a case of omitted variable bias. Section 4.5 Below, the R output from the ECLS example is reported. Please provide a written explanation of the regression coefficients for SES and ATL, using the interpretations from Section?? and / or any other interpretations you want to talk about. If you have questions about how to interpret the coefficients, also note them now. And, please be prepared to share your thoughts in class! summary(mod1) ## ## Call: ## lm(formula = Math ~ SES + ATL, data = example_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.10 -4.03 -1.07 3.29 29.54 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.0502 2.9003 -2.09 0.038 * ## SES 0.3512 0.0563 6.24 1.9e-09 *** ## ATL 3.5212 0.6739 5.23 3.7e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.2 on 247 degrees of freedom ## Multiple R-squared: 0.273, Adjusted R-squared: 0.267 ## F-statistic: 46.3 on 2 and 247 DF, p-value: &lt;2e-16 Section 4.6 Please write down an interpretation of the of beta coefficients in the above below. Your interpretation should include reference to the fact that the variables have been standardized. Please be prepared to share your interpretation / questions in class! summary(z_mod) ## ## Call: ## lm(formula = Math ~ SES + ATL, data = z_example_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.959 -0.560 -0.149 0.457 4.104 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.22e-16 5.42e-02 0.00 1 ## SES 3.53e-01 5.67e-02 6.24 1.9e-09 *** ## ATL 2.96e-01 5.67e-02 5.23 3.7e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.86 on 247 degrees of freedom ## Multiple R-squared: 0.273, Adjusted R-squared: 0.267 ## F-statistic: 46.3 on 2 and 247 DF, p-value: &lt;2e-16 Section 4.7 The R-squared for the ECLS example is equal to .273. Please write down your interpretation of this value and be prepared to share your answer in class. Section 4.8 Look at the R output for the ECLS example above again (either one). Please write down your conclusions about the statistical significance of the predictors and be prepared to share your answer in class. Section 4.9 Look at the R output for the ECLS example above again (either one). Please write down your conclusion about the statistical significance of the R-squared statistic in the ECLS example. "],["4.11-exercises-1.html", "4.11 Exercises", " 4.11 Exercises These notes provide an overview of regression with two variables in R. 4.11.1 The ECLS250 data Let’s start by getting our example data loaded into R. We will be using a subset of \\(N = 250\\) cases from the Early Childhood Longitudinal Survey 1998-1998 (ECLS-K). Here is a description of the data from the official NCES codebook (page 1-1 of https://nces.ed.gov/ecls/data/ECLSK_K8_Manual_part1.pdf): The ECLS-K focuses on children’s early school experiences beginning with kindergarten and ending with eighth grade. It is a multisource, multimethod study that includes interviews with parents, the collection of data from principals and teachers, and student records abstracts, as well as direct child assessments. In the eighth-grade data collection, a student paper-and-pencil questionnaire was added. The ECLS-K was developed under the sponsorship of the U.S. Department of Education, Institute of Education Sciences, National Center for Education Statistics (NCES). Westat conducted this study with assistance provided by Educational Testing Service (ETS) in Princeton, New Jersey. The ECLS-K followed a nationally representative cohort of children from kindergarten into middle school. The base-year data were collected in the fall and spring of the 1998–99 school year when the sampled children were in kindergarten. A total of 21,260 kindergartners throughout the nation participated. The subset of the ECLS-K data used in this class was obtained from the link below. The codebook for these data is available in our Resources folder. Note that we will be using only a small subset of the full ECLS2577 data for this example http://routledgetextbooks.com/textbooks/_author/ware-9780415996006/data.php Let’s load ECLS-K data into R. Make sure to download the file ECLS250.RData from this week’s resources folder and save the file in your working directory – check out the R exercises from our first lesson for a refresher of how to do this. detach(ecls) # removed previously attached ecls data load(&quot;ECLS250.RData&quot;) # load new example attach(ecls) # attach # knitr and kable are just used to print nicely -- you can just use head(ecls[, 1:5]) knitr::kable(head(ecls[, 1:5])) caseid gender race c1rrscal c1rrttsco 960 2 1 28 58 113 1 8 14 39 1828 1 1 22 50 1693 1 1 21 50 643 2 1 14 39 772 1 1 21 49 The naming conventions for these data are bit challenging. Variable names begin with c, p, or t depending on whether the respondent was the child, parent, or teacher. Variables that start with wk were created by the ECLS using other data sources available in during the kindergarten year of the study. The time points (1-4 denoting fall and spring of K and Gr 1) appear as the second character. The rest of the name describes the variable. The variables we will use for this illustration are: c1rmscal: Child’s score on a math assessment, in first semester of Kindergarten . The scores can be interpreted as number of correct responses out of a total of approximately 80 math exam questions. wksesl: An SES composite of household factors (e.g., parental education, household income) ranging from 30-72. t1learn: Approaches to Learning Scale (ATLS), teacher reported in first semester of kindergarten. This scale measures behaviors that affect the ease with which children can benefit from the learning environment. It includes six items that rate the child’s attentiveness, task persistence, eagerness to learn, learning independence, flexibility, and organization. The items have 4 response categories (1-4), coded so that higher values represent more positive responses, and the scale is an unweighted average the six items. To get started lets produce the simple regression of Math with SES. This is another look at the relationship between Academic Achievement and SES that we discussed in Chapter 2. If you do not feel comfortable running this analysis or interpreting the output, take another look at Section 5.7. plot(x = wksesl, y = c1rmscal, col = &quot;#4B9CD3&quot;) mod &lt;- lm(c1rmscal ~ wksesl) abline(mod) summary(mod) ## ## Call: ## lm(formula = c1rmscal ~ wksesl) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.131 -4.355 -0.849 3.678 31.536 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.6159 2.7393 0.22 0.82 ## wksesl 0.4359 0.0567 7.68 3.6e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.5 on 248 degrees of freedom ## Multiple R-squared: 0.192, Adjusted R-squared: 0.189 ## F-statistic: 59 on 1 and 248 DF, p-value: 3.61e-13 cor(wksesl, c1rmscal) ## [1] 0.44 4.11.2 Multiple regression with lm Let’s tale a look at “zero-order” relationship among the three variables. This type of descriptive, two-way analysis is a good way to get familiar with your data before getting into multiple regression analysis. We can see that the variables are all moderately correlated and their relationships appear reasonably linear. # Use cbind to create a data.frame with just the 3 variables we want to examine data &lt;- cbind(c1rmscal, wksesl, t1learn) # Correlations cor(data) ## c1rmscal wksesl t1learn ## c1rmscal 1.00 0.44 0.40 ## wksesl 0.44 1.00 0.29 ## t1learn 0.40 0.29 1.00 # Scatterplots pairs(data, col = &quot;#4B9CD3&quot;) In terms of input, multiple regression with lm is just as simple as for a single predictor. The only difference is the model formula. To include more predictors in a formula, just include them on the right hand side, separated by at + sign. e.g, Y ~ Χ1 + Χ2 For our example, let’s consider the regression of math achievement on SES and Approaches to Learning. We’ll save our result as mod1 which is short for “model one.” mod1 &lt;- lm(c1rmscal ~ wksesl + t1learn) summary(mod1) ## ## Call: ## lm(formula = c1rmscal ~ wksesl + t1learn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.10 -4.03 -1.07 3.29 29.54 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.0502 2.9003 -2.09 0.038 * ## wksesl 0.3512 0.0563 6.24 1.9e-09 *** ## t1learn 3.5212 0.6739 5.23 3.7e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.2 on 247 degrees of freedom ## Multiple R-squared: 0.273, Adjusted R-squared: 0.267 ## F-statistic: 46.3 on 2 and 247 DF, p-value: &lt;2e-16 We can see from the output that regression coefficient for t1learn is about 3.5. This means that, as the predictor increases by a single unit, children’s predicted math scores increase by 3.5 points (out of 80), after controlling for the SES. You should be able to provide a similar interpretation of the regression coefficient for wksesl. Together, both predictors accounted for about 27% of the variation in students’ math scores. In education, this would be considered a pretty good coefficient of determination. We will talk about the statistical tests later on. For now let’s consider the relationship with simple regression. 4.11.3 Relations between simple and multiple regression First let’s consider how the two simple regression compare to the multiple regression with two variables. Here is the relevant output: # Compare the multiple regression output to the simple regressions mod2a &lt;- lm(c1rmscal ~ wksesl) summary(mod2a) ## ## Call: ## lm(formula = c1rmscal ~ wksesl) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.131 -4.355 -0.849 3.678 31.536 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.6159 2.7393 0.22 0.82 ## wksesl 0.4359 0.0567 7.68 3.6e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.5 on 248 degrees of freedom ## Multiple R-squared: 0.192, Adjusted R-squared: 0.189 ## F-statistic: 59 on 1 and 248 DF, p-value: 3.61e-13 mod2b &lt;- lm(c1rmscal ~ t1learn) summary(mod2b) ## ## Call: ## lm(formula = c1rmscal ~ t1learn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.40 -4.21 -1.00 3.77 31.84 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.039 2.148 3.28 0.0012 ** ## t1learn 4.730 0.693 6.83 6.7e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.6 on 248 degrees of freedom ## Multiple R-squared: 0.158, Adjusted R-squared: 0.155 ## F-statistic: 46.6 on 1 and 248 DF, p-value: 6.66e-11 The important things to note here are The regression coefficients from the simple models (\\(b_{ses} = 4.38\\) and \\(b_{t1learn} = 4.73\\)) are larger than the regression coefficients from the two-predictor model. Can you explain why? (Hint: see Section 4.5.) The R-squared terms in the two simple models (.194 + .158 = .352) add up to more than the R-squared in the two-predictor model (.274). Again, take a moment to think about why before reading on. (Hint: see Section 4.7.) 4.11.4 Inference with 2 predictors Let’s move on now to consider the statistical tests and confidence intervals provided with the lm summary output. For regression with more than one predictor, both the t-tests and F-tests have a very similar construction and interpretation as with simple regression. The main differences are (see Sections 4.8 and 4.9): The degrees of freedom for both tests now involve \\(J\\), the number of predictors. The standard error of the b-weight is more complicated, because it involves the inter-correlation among the predictors. We can see that for mod1 that both b-weights are significant at the .05 level, and so is the R-square. As mentioned previously, it is not usual to interpret or report results on the regression intercept unless you have a special reason to do so (e.g., see Chapter 5). # Revisting the output of mod1 summary(mod1) ## ## Call: ## lm(formula = c1rmscal ~ wksesl + t1learn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.10 -4.03 -1.07 3.29 29.54 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.0502 2.9003 -2.09 0.038 * ## wksesl 0.3512 0.0563 6.24 1.9e-09 *** ## t1learn 3.5212 0.6739 5.23 3.7e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.2 on 247 degrees of freedom ## Multiple R-squared: 0.273, Adjusted R-squared: 0.267 ## F-statistic: 46.3 on 2 and 247 DF, p-value: &lt;2e-16 4.11.5 APA reporting of results In terms of writing out the results, there are many formatting styles used in social sciences. As one example, the convention for APA style is to write the coefficient, followed by the test statistic (with its degrees of freedom) and then the p-value. It is also conventional to use 2 decimal places, unless more decimal places are needed to address rounding error. Here is how we might write out the results of our regression using APA format: The regression of Math Achievement on SES was positive and statistically significant at the .05 level (\\(b = 3.53, t(247) = 6.27, p &lt; .001\\)). The regression of Math Achievement on Approaches to Learning was also positive and statistically significant at the .05 level (\\(b = 3.50, t(247) = 5.20, p &lt; .001\\)). Together both predictors accounted for about 27% of the variation in Math Achievement (\\(R^2\\) = .274, adjusted \\(R^2\\) = .268), which was also statistically significant at the .05 level (\\(F(2, 247) = 45.54, p &lt; .001\\)). Instead of, or in addition to, the statistical tests, we could include the confidence intervals for the regression coefficients. It is not usual to report confidence intervals on R-squared. confint(mod1) ## 2.5 % 97.5 % ## (Intercept) -11.76 -0.34 ## wksesl 0.24 0.46 ## t1learn 2.19 4.85 The 95% confidence interval on the regression coefficient of Math achievement on SES was \\([2.42 , 4.64]\\). For Approaches to Learning, the 95% confidence interval was \\([2.18, 4.83]\\). When we have a regression model with many predictors, or are comparing among different models, it is more usual to put all the relevant statistics in a table rather than writing them out one by one. We will see how to do that later on in the course. For more info on APA format, see the APA publications manual (https://www.apastyle.org/manual). "]]
